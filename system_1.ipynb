{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/pytorch_LSTM_1.png\" width=\"550\">\n",
    "\n",
    "### Define LSTM architecture：\n",
    ">1. input_size: 每个单词的 embedding or one-hot encoding 的尺寸。\n",
    "2. hidden_size: 隐含层神经元的个数。\n",
    "3. num_layers: LSTM的层数。\n",
    "4. bias:\n",
    "5. batch_first: \n",
    "6. dropout: 0-1\n",
    "7. bidirectional: true or false.\n",
    "\n",
    "### LSTM input：\n",
    ">\n",
    "### LSTM output：\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-processed dataset.\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch   \n",
    "from torchtext import data \n",
    "import random\n",
    "import os\n",
    "from torchtext.vocab import Vectors\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37293"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"pre-processed data/new_label_dataset.csv\", usecols=['content', 'categories', 'priority'])\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': ['philippine', 'flood', 'worsen', 'death', 'toll', 'hit', 'wake', 'gener'], 'priority': 'Low'}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2020)\n",
    "\n",
    "# loading custom dataset\n",
    "TEXT = data.Field(tokenize=lambda x: x.split() ,batch_first=True, include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float, batch_first=True)\n",
    "\n",
    "fields = [(None, None), ('content', TEXT), (None, None), ('priority', LABEL)]\n",
    "\n",
    "dataset=data.TabularDataset(path = 'pre-processed data/new_label_dataset.csv',format = 'csv',fields = fields,skip_header = True)\n",
    "\n",
    "print(vars(dataset.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 6543\n",
      "Size of LABEL vocabulary: 5\n",
      "Top words:  [('shoot', 3323), ('earthquake', 2530), ('people', 2370), ('philippine', 2132), ('school', 2119)]\n",
      "LABEL vocabulary:  defaultdict(None, {'Low': 0, 'Medium': 1, 'High': 2, 'Critical': 3, 'Unknown': 4})\n"
     ]
    }
   ],
   "source": [
    "tr_X, te_X = dataset.split(split_ratio=0.8, random_state = random.seed(2020))\n",
    "tr_x, val_x = tr_X.split(split_ratio=0.7, random_state = random.seed(2020))\n",
    "\n",
    "# load downloaded glove word embedding.\n",
    "cache = '.vector_cache'\n",
    "if not os.path.exists(cache): os.mkdir(cache)\n",
    "vectors = Vectors(name='./glove.840B.300d.txt', cache=cache)\n",
    "\n",
    "# create vocab.\n",
    "TEXT.build_vocab(tr_X, min_freq=3, vectors=vectors)\n",
    "LABEL.build_vocab(tr_X)\n",
    "\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "print(\"Top words: \", TEXT.vocab.freqs.most_common(5))  \n",
    "\n",
    "# Word dictionary.\n",
    "print(\"LABEL vocabulary: \", LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "#set batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((tr_x, val_x), \n",
    "                                                            batch_size = BATCH_SIZE,\n",
    "                                                            sort_key = lambda x: len(x.content),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classifier(nn.Module):\n",
    "    #define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # bi-directional LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=bidirectional, \n",
    "                            dropout=dropout,\n",
    "                            batch_first=True)\n",
    "\n",
    "        #dense layer\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "\n",
    "        #activation function\n",
    "        self.act = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        \n",
    "        #concat the final forward and backward hidden state\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        dense_outputs=self.fc(hidden)\n",
    "        \n",
    "        #Final activation function\n",
    "        outputs=self.act(dense_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    #initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        #retrieve text and no. of words\n",
    "        cont, cont_lengths = batch.content \n",
    "\n",
    "        #convert to 1D tensor\n",
    "        predictions = model(cont, cont_lengths).squeeze()  \n",
    "        \n",
    "        #compute the loss\n",
    "        loss = criterion(predictions, batch.priority.long())  \n",
    "        \n",
    "        # backward propagation\n",
    "        loss.backward()       \n",
    "        \n",
    "        #update the weights\n",
    "        optimizer.step() \n",
    "        \n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()     \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    # initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    # deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            # retrieve text and length\n",
    "            cont, cont_lengths = batch.content \n",
    "\n",
    "            predictions = model(cont, cont_lengths).squeeze() \n",
    "            \n",
    "            # compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.priority.long())\n",
    "            \n",
    "            # train loss.\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_vocab = len(TEXT.vocab)\n",
    "\n",
    "def Adam(n_epoches=100, lr = 0.001, dropout = 0.3, num_hidden_nodes = 32):\n",
    "    \n",
    "    train_loss_list, valid_loss_list = [], []\n",
    "    \n",
    "    embedding_dim = 300\n",
    "    num_output_nodes = 5\n",
    "    num_layers = 1 # depth\n",
    "    bidirection = True\n",
    "\n",
    "    # RNN model\n",
    "    model = RNN_classifier(size_of_vocab, embedding_dim, \n",
    "                       num_hidden_nodes,num_output_nodes, \n",
    "                       num_layers, bidirectional = True, \n",
    "                       dropout = dropout)\n",
    "    print(model)\n",
    "    \n",
    "    # pre-trained Glove.\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "    \n",
    "    # define optimizer and loss.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # training\n",
    "    for epoch in range(n_epoches):\n",
    "\n",
    "        #train the model\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "\n",
    "        #evaluate the model\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "\n",
    "        print(f\"Epoch: {epoch:d} : Train Loss: {train_loss:.4f} | Valid loss: {valid_loss:.4f}\")\n",
    "        \n",
    "    return train_loss_list, valid_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_classifier(\n",
      "  (embedding): Embedding(6543, 300)\n",
      "  (lstm): LSTM(300, 20, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (fc): Linear(in_features=40, out_features=5, bias=True)\n",
      "  (act): Softmax(dim=1)\n",
      ")\n",
      "Epoch: 0 : Train Loss: 1.2181 | Valid loss: 1.1753\n",
      "Epoch: 1 : Train Loss: 1.1758 | Valid loss: 1.1746\n",
      "Epoch: 2 : Train Loss: 1.1753 | Valid loss: 1.1746\n",
      "Epoch: 3 : Train Loss: 1.1751 | Valid loss: 1.1751\n",
      "Epoch: 4 : Train Loss: 1.1602 | Valid loss: 1.1576\n",
      "Epoch: 5 : Train Loss: 1.1383 | Valid loss: 1.1539\n",
      "Epoch: 6 : Train Loss: 1.1179 | Valid loss: 1.1495\n",
      "Epoch: 7 : Train Loss: 1.0971 | Valid loss: 1.1513\n",
      "Epoch: 8 : Train Loss: 1.0819 | Valid loss: 1.1534\n",
      "Epoch: 9 : Train Loss: 1.0719 | Valid loss: 1.1524\n",
      "Epoch: 10 : Train Loss: 1.0644 | Valid loss: 1.1529\n",
      "Epoch: 11 : Train Loss: 1.0583 | Valid loss: 1.1556\n",
      "Epoch: 12 : Train Loss: 1.0549 | Valid loss: 1.1638\n",
      "Epoch: 13 : Train Loss: 1.0515 | Valid loss: 1.1590\n",
      "Epoch: 14 : Train Loss: 1.0485 | Valid loss: 1.1621\n",
      "Epoch: 15 : Train Loss: 1.0466 | Valid loss: 1.1582\n",
      "Epoch: 16 : Train Loss: 1.0439 | Valid loss: 1.1582\n",
      "Epoch: 17 : Train Loss: 1.0421 | Valid loss: 1.1591\n",
      "Epoch: 18 : Train Loss: 1.0410 | Valid loss: 1.1604\n",
      "Epoch: 19 : Train Loss: 1.0397 | Valid loss: 1.1651\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = Adam(n_epoches=20, lr=0.001, dropout=0.3, num_hidden_nodes = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
