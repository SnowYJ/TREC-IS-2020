{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## traditional machine learning classifier - sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42762\n",
      "42761\n"
     ]
    }
   ],
   "source": [
    "# load processed dataset.\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"pre-processed data/dataset_aug_priority.csv\", sep=',')\n",
    "print(len(dataset))\n",
    "dataset = dataset[dataset['priority'] != 'Unknown']\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# create vocab according to tf.\n",
    "count_vec = CountVectorizer(ngram_range=(1, 1), max_features=3000)\n",
    "tf = count_vec.fit_transform(dataset['content'].values.astype('U')).toarray()\n",
    "vocab = count_vec.get_feature_names()\n",
    "\n",
    "# td.idf\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 1), min_df=1, vocabulary=vocab)\n",
    "tfidf= tfidf_vec.fit_transform(dataset['content'].values.astype('U')).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abasand</th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>above</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abundance</th>\n",
       "      <th>...</th>\n",
       "      <th>your</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yukon</th>\n",
       "      <th>zambales</th>\n",
       "      <th>zero</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zone</th>\n",
       "      <th>zulu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ab  abasand  abc  ability  able  above  abroad  absolute  absolutely  \\\n",
       "0  0.0      0.0  0.0      0.0   0.0    0.0     0.0       0.0         0.0   \n",
       "1  0.0      0.0  0.0      0.0   0.0    0.0     0.0       0.0         0.0   \n",
       "2  0.0      0.0  0.0      0.0   0.0    0.0     0.0       0.0         0.0   \n",
       "3  0.0      0.0  0.0      0.0   0.0    0.0     0.0       0.0         0.0   \n",
       "4  0.0      0.0  0.0      0.0   0.0    0.0     0.0       0.0         0.0   \n",
       "\n",
       "   abundance  ...  your  yourself  youtube   yr  yukon  zambales  zero  \\\n",
       "0        0.0  ...   0.0       0.0      0.0  0.0    0.0       0.0   0.0   \n",
       "1        0.0  ...   0.0       0.0      0.0  0.0    0.0       0.0   0.0   \n",
       "2        0.0  ...   0.0       0.0      0.0  0.0    0.0       0.0   0.0   \n",
       "3        0.0  ...   0.0       0.0      0.0  0.0    0.0       0.0   0.0   \n",
       "4        0.0  ...   0.0       0.0      0.0  0.0    0.0       0.0   0.0   \n",
       "\n",
       "   zimbabwe  zone  zulu  \n",
       "0       0.0   0.0   0.0  \n",
       "1       0.0   0.0   0.0  \n",
       "2       0.0   0.0   0.0  \n",
       "3       0.0   0.0   0.0  \n",
       "4       0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 3000 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tfidf = pd.DataFrame(tfidf, columns=vocab)\n",
    "\n",
    "dataset_tfidf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34209\n",
      "34209\n",
      "8552\n",
      "8552\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# split into train and test dataset.\n",
    "tr_X = dataset_tfidf.sample(frac=0.8, random_state=2020)\n",
    "tr_y = dataset.sample(frac=0.8, random_state=2020)['priority']\n",
    "te_X = dataset_tfidf[~dataset_tfidf.index.isin(tr_X.index)]\n",
    "te_y = dataset[~dataset_tfidf.index.isin(tr_X.index)]['priority']\n",
    "\n",
    "tr_X, tr_y = np.array(tr_X), np.array(tr_y)\n",
    "te_X, te_y = np.array(te_X), np.array(te_y)\n",
    "\n",
    "print(len(tr_X))\n",
    "print(len(tr_y))\n",
    "print(len(te_X))\n",
    "print(len(te_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-class classification for 'priority':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y[tr_y == 'Critical'], te_y[te_y == 'Critical'] = 0, 0\n",
    "tr_y[tr_y == 'Low'], te_y[te_y == 'Low'] = 1, 1\n",
    "tr_y[tr_y == 'Medium'], te_y[te_y == 'Medium'] = 2, 2\n",
    "tr_y[tr_y == 'High'], te_y[te_y == 'High'] = 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error: Unknown label type: 'unknown'\n",
    "tr_y=tr_y.astype('int')\n",
    "te_y=te_y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7904583723105706\n",
      "Precision: 0.8003093310868893\n",
      "Recall: 0.8113226037861101\n",
      "F1-Score: 0.8048295539715175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# logistic regression.\n",
    "clf = LogisticRegression(random_state=0, solver='saga', multi_class='auto').fit(tr_X, tr_y)\n",
    "preds_te = clf.predict(te_X)\n",
    "\n",
    "print('Accuracy:', accuracy_score(te_y,preds_te))\n",
    "print('Precision:', precision_score(te_y,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(te_y,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(te_y,preds_te,average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-label classification for 'Categories': one-vs-rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>philippine flood worsen death toll hit wake ge...</td>\n",
       "      <td>['ThirdPartyObservation', 'Factoid', 'News']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>philippine flood fatality hit</td>\n",
       "      <td>['ThirdPartyObservation', 'Factoid', 'News']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>luzon dam release water flood warn up manila p...</td>\n",
       "      <td>['ThirdPartyObservation', 'Factoid', 'News']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pagasa advisory yellow warning metro manila oc...</td>\n",
       "      <td>['ThirdPartyObservation', 'Factoid', 'News']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  philippine flood worsen death toll hit wake ge...   \n",
       "1                     philippine flood fatality hit    \n",
       "2  luzon dam release water flood warn up manila p...   \n",
       "3  pagasa advisory yellow warning metro manila oc...   \n",
       "\n",
       "                                     categories  \n",
       "0  ['ThirdPartyObservation', 'Factoid', 'News']  \n",
       "1  ['ThirdPartyObservation', 'Factoid', 'News']  \n",
       "2  ['ThirdPartyObservation', 'Factoid', 'News']  \n",
       "3  ['ThirdPartyObservation', 'Factoid', 'News']  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load processed dataset.\n",
    "dataset_cat = pd.read_csv(\"pre-processed data/dataset_aug_cat.csv\")[['content', 'categories']]\n",
    "\n",
    "dataset_cat[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53785, 25)\n"
     ]
    }
   ],
   "source": [
    "# convert categories into matrix.\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# convert str to list.\n",
    "cat_list = []\n",
    "for i in dataset_cat['categories']:\n",
    "    cat_list.append(ast.literal_eval(i))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(cat_list)\n",
    "categories = mlb.classes_\n",
    "\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53785, 3000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create vocab according to tf.\n",
    "count_vec = CountVectorizer(ngram_range=(1, 1), max_features=3000)\n",
    "tf = count_vec.fit_transform(dataset_cat['content'].values.astype('U')).toarray()\n",
    "vocab = count_vec.get_feature_names()\n",
    "\n",
    "# td.idf\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 1), min_df=1, vocabulary=vocab)\n",
    "tfidf= tfidf_vec.fit_transform(dataset_cat['content'].values.astype('U')).toarray()\n",
    "\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_X size:  (37649, 3000)\n",
      "tr_y size:  (37649, 25)\n",
      "te_X size:  (16136, 3000)\n",
      "te_y size:  (16136, 25)\n"
     ]
    }
   ],
   "source": [
    "# random split into train and test dataset.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr_X, te_X, tr_y, te_y = train_test_split(tfidf, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"tr_X size: \", tr_X.shape)\n",
    "print(\"tr_y size: \", tr_y.shape)\n",
    "print(\"te_X size: \", te_X.shape)\n",
    "print(\"te_y size: \", te_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Advice  F1-score: 0.23581616481774959\n",
      "Category: CleanUp  F1-score: 0.31683168316831684\n",
      "Category: ContextualInformation  F1-score: 0.3273185060847671\n",
      "Category: Discussion  F1-score: 0.25735797399041754\n",
      "Category: Donations  F1-score: 0.8842201415848366\n",
      "Category: EmergingThreats  F1-score: 0.26729665879176506\n",
      "Category: Factoid  F1-score: 0.5683579985905567\n",
      "Category: FirstPartyObservation  F1-score: 0.3392239023501548\n",
      "Category: GoodsServices  F1-score: 0.7409700722394221\n",
      "Category: Hashtags  F1-score: 0.5467997651203758\n",
      "Category: InformationWanted  F1-score: 0.8336594911937377\n",
      "Category: Irrelevant  F1-score: 0.5298820978452364\n",
      "Category: Location  F1-score: 0.6505177514792899\n",
      "Category: MovePeople  F1-score: 0.7370644139387541\n",
      "Category: MultimediaShare  F1-score: 0.530171277997365\n",
      "Category: NewSubEvent  F1-score: 0.6355394378966456\n",
      "Category: News  F1-score: 0.5762843798650752\n",
      "Category: Official  F1-score: 0.26732673267326734\n",
      "Category: OriginalEvent  F1-score: 0.3607843137254902\n",
      "Category: SearchAndRescue  F1-score: 0.8585434173669468\n",
      "Category: Sentiment  F1-score: 0.6227892142650043\n",
      "Category: ServiceAvailable  F1-score: 0.26600090785292785\n",
      "Category: ThirdPartyObservation  F1-score: 0.49990904129525204\n",
      "Category: Volunteer  F1-score: 0.7221350078492935\n",
      "Category: Weather  F1-score: 0.5241650294695481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "one_vs_rest = OneVsRestClassifier(LogisticRegression(class_weight=\"balanced\", random_state=0, solver='lbfgs', max_iter=200), n_jobs=1)\n",
    "preds_te_arr = np.zeros(te_y.shape) \n",
    "for i, category in enumerate(categories):\n",
    "    \n",
    "    one_vs_rest.fit(tr_X, tr_y[:, i])\n",
    "    \n",
    "    preds_te = one_vs_rest.predict(te_X)\n",
    "    preds_te_arr[:, i] = preds_te\n",
    "    print('Category:', category, ' F1-score:', f1_score(te_y[:, i], preds_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7904300787446569\n",
      "F1-Score: 0.523958615258088\n"
     ]
    }
   ],
   "source": [
    "print('Recall:', recall_score(te_y,preds_te_arr,average='macro'))\n",
    "print('F1-Score:', f1_score(te_y,preds_te_arr,average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
