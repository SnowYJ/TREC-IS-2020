{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-label tweets classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch   \n",
    "from torchtext import data \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "import os\n",
    "from torchtext.vocab import Vectors # downloaded word embedding\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from ast import literal_eval # convert string to list\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import re\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"pre-processed data/dataset_aug_cat.csv\")[['content', 'categories']]\n",
    "\n",
    "# convert string to list.\n",
    "dataset['categories'] = dataset['categories'].apply(lambda x:literal_eval(x))\n",
    "\n",
    "# seperate 'categories' column.\n",
    "cat_list = []\n",
    "for i in dataset['categories']:\n",
    "    cat_list.append(i)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(cat_list)\n",
    "categories = mlb.classes_\n",
    "\n",
    "# convert list to columns.\n",
    "for i, cat in enumerate(categories):\n",
    "    dataset[cat] = labels[:, i]\n",
    "    \n",
    "dataset.to_csv(\"pre-processed data/new_label_dataset_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0190, 0.0030, 0.0175, 0.0285, 0.0642, 0.0182, 0.0628, 0.0422, 0.0125,\n",
       "        0.0926, 0.0268, 0.0806, 0.0640, 0.0120, 0.0851, 0.0259, 0.0904, 0.0144,\n",
       "        0.0316, 0.0197, 0.0858, 0.0129, 0.0576, 0.0085, 0.0242],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for imbalance dataset.\n",
    "weight = torch.tensor(labels.sum(axis=0)/sum(labels.sum(axis=0)))\n",
    "\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>categories</th>\n",
       "      <th>Advice</th>\n",
       "      <th>CleanUp</th>\n",
       "      <th>ContextualInformation</th>\n",
       "      <th>Discussion</th>\n",
       "      <th>Donations</th>\n",
       "      <th>EmergingThreats</th>\n",
       "      <th>Factoid</th>\n",
       "      <th>FirstPartyObservation</th>\n",
       "      <th>...</th>\n",
       "      <th>NewSubEvent</th>\n",
       "      <th>News</th>\n",
       "      <th>Official</th>\n",
       "      <th>OriginalEvent</th>\n",
       "      <th>SearchAndRescue</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ServiceAvailable</th>\n",
       "      <th>ThirdPartyObservation</th>\n",
       "      <th>Volunteer</th>\n",
       "      <th>Weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>philippine flood worsen death toll hit wake ge...</td>\n",
       "      <td>[ThirdPartyObservation, Factoid, News]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  philippine flood worsen death toll hit wake ge...   \n",
       "\n",
       "                               categories  Advice  CleanUp  \\\n",
       "0  [ThirdPartyObservation, Factoid, News]       0        0   \n",
       "\n",
       "   ContextualInformation  Discussion  Donations  EmergingThreats  Factoid  \\\n",
       "0                      0           0          0                0        1   \n",
       "\n",
       "   FirstPartyObservation  ...  NewSubEvent  News  Official  OriginalEvent  \\\n",
       "0                      0  ...            0     1         0              0   \n",
       "\n",
       "   SearchAndRescue  Sentiment  ServiceAvailable  ThirdPartyObservation  \\\n",
       "0                0          0                 0                      1   \n",
       "\n",
       "   Volunteer  Weather  \n",
       "0          0        0  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchtext:\n",
    "<img src=\"image/torchtext.png\" width=\"600\">\n",
    "\n",
    "torchtext.data\n",
    ">\n",
    ">* Field: \n",
    "* TabularDataset: \n",
    "* Example\n",
    "* Iterator (Iterator, BucketIterator, BPTTIterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': ['philippine', 'flood', 'worsen', 'death', 'toll', 'hit', 'wake', 'gener'], 'Advice': '0', 'CleanUp': '0', 'ContextualInformation': '0', 'Discussion': '0', 'Donations': '0', 'EmergingThreats': '0', 'Factoid': '1', 'FirstPartyObservation': '0', 'GoodsServices': '0', 'Hashtags': '0', 'InformationWanted': '0', 'Irrelevant': '0', 'Location': '0', 'MovePeople': '0', 'MultimediaShare': '0', 'NewSubEvent': '0', 'News': '1', 'Official': '0', 'OriginalEvent': '0', 'SearchAndRescue': '0', 'Sentiment': '0', 'ServiceAvailable': '0', 'ThirdPartyObservation': '1', 'Volunteer': '0', 'Weather': '0'}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2020)\n",
    "\n",
    "# the same as system_2.\n",
    "TEXT = data.Field(sequential=True, tokenize=lambda x: x.split(), lower=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "fields = [(v, LABEL) for v in categories]\n",
    "\n",
    "fields = [(None, None), ('content', TEXT), (None, None)] + fields\n",
    "\n",
    "dataset=data.TabularDataset(path = 'pre-processed data/new_label_dataset_1.csv',format = 'csv',fields = fields,skip_header = True)\n",
    "\n",
    "print(vars(dataset.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use downloaded word embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 7252\n",
      "Top words:  [('nepal', 4937), ('help', 3879), ('earthquake', 3486), ('shoot', 3411), ('people', 2972)]\n"
     ]
    }
   ],
   "source": [
    "tr_X, te_X = dataset.split(split_ratio=0.8, random_state = random.seed(2020))\n",
    "tr_x, val_x = tr_X.split(split_ratio=0.7, random_state = random.seed(2020))\n",
    "\n",
    "# load downloaded glove word embedding.\n",
    "cache = '.vector_cache'\n",
    "if not os.path.exists(cache): os.mkdir(cache)\n",
    "vectors = Vectors(name='./glove.840B.300d.txt', cache=cache)\n",
    "\n",
    "# create vocab.\n",
    "TEXT.build_vocab(tr_X, min_freq=3, vectors=vectors)\n",
    "\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "print(\"Top words: \", TEXT.vocab.freqs.most_common(5))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BucketIterator:\n",
    "\n",
    "通过sort_key，BucketIterator将长度相近的数据放到同一个batch内来sample，这样可以最小化每个batch需要padding的个数，提高计算效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((tr_X, te_X), \n",
    "                                                            batch_sizes = (64, 64),\n",
    "                                                            sort_key = lambda x: len(x.content),\n",
    "                                                            sort_within_batch=False,\n",
    "                                                            repeat=False,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python - yield:\n",
    "\n",
    "**yield** keyword means this function is a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "            self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # x_var is content, y_var is label.\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # return value of x_var \"content\" in object batch.\n",
    "            y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            yield (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "train_dl = BatchWrapper(train_iterator, \"content\", list(categories))\n",
    "valid_dl = BatchWrapper(valid_iterator, \"content\", list(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.RELU and F.RELU\n",
    "\n",
    "nn.ReLU作为一个层结构，必须添加到nn.Module容器中才能使用，而F.ReLU则作为一个函数调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, hidden_dim=30, emb_dim=300):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=1, dropout=0.2, bidirectional = True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 64)\n",
    "        self.fc1 = nn.Linear(64, 25)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        embed = self.embedding(seq)\n",
    "        _, hidden = self.gru(embed)\n",
    "        embed1 = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        output = self.fc(embed1)\n",
    "        output1 = self.fc1(F.relu(output))\n",
    "        preds = output1 # BCEWithlogitsLoss = sigmoid + BCELoss\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduction = 'none', 'elementwise_mean' and 'sum'\n",
    "\n",
    ">* elementwise_mean: (default) the average of sum of loss.\n",
    "* sum: the sum of loss.\n",
    "* none: don't process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  0.3089557653860387  | Validation Loss:  0.2607326168104036\n",
      "Epoch:  1  Training Loss:  0.2541649442990857  | Validation Loss:  0.25984266320981925\n",
      "Epoch:  2  Training Loss:  0.25183350246338754  | Validation Loss:  0.25849292943110835\n",
      "Epoch:  3  Training Loss:  0.24796207018894037  | Validation Loss:  0.25617814487254126\n",
      "Epoch:  4  Training Loss:  0.24194755275660426  | Validation Loss:  0.2515416602940249\n",
      "Epoch:  5  Training Loss:  0.23626670148362022  | Validation Loss:  0.2473175660569287\n",
      "Epoch:  6  Training Loss:  0.23212100363946775  | Validation Loss:  0.24310403947646803\n",
      "Epoch:  7  Training Loss:  0.22843348322350415  | Validation Loss:  0.23801832359570724\n",
      "Epoch:  8  Training Loss:  0.2250609983528174  | Validation Loss:  0.2329971644829011\n",
      "Epoch:  9  Training Loss:  0.22216781854275255  | Validation Loss:  0.22986725052080212\n",
      "Epoch:  10  Training Loss:  0.21981724837648214  | Validation Loss:  0.22760144167045165\n",
      "Epoch:  11  Training Loss:  0.21779864672211624  | Validation Loss:  0.22455185651779175\n",
      "Epoch:  12  Training Loss:  0.21589400850646032  | Validation Loss:  0.22168486948902086\n",
      "Epoch:  13  Training Loss:  0.2140347766973643  | Validation Loss:  0.2219730994757816\n",
      "Epoch:  14  Training Loss:  0.2122088741140238  | Validation Loss:  0.21811276409752975\n",
      "Epoch:  15  Training Loss:  0.21050136291697333  | Validation Loss:  0.21684323797917224\n",
      "Epoch:  16  Training Loss:  0.20885299188244041  | Validation Loss:  0.213746872350309\n",
      "Epoch:  17  Training Loss:  0.2071635092748398  | Validation Loss:  0.21303992043938158\n",
      "Epoch:  18  Training Loss:  0.20555405214672456  | Validation Loss:  0.2105188082308459\n",
      "Epoch:  19  Training Loss:  0.2038943136960578  | Validation Loss:  0.20896261859927656\n",
      "Epoch:  20  Training Loss:  0.20229113314516653  | Validation Loss:  0.20740537213150567\n",
      "Epoch:  21  Training Loss:  0.20074402506602998  | Validation Loss:  0.20635365267124403\n",
      "Epoch:  22  Training Loss:  0.19916520682876213  | Validation Loss:  0.20456144319483516\n",
      "Epoch:  23  Training Loss:  0.1976475817930468  | Validation Loss:  0.20460250003803412\n",
      "Epoch:  24  Training Loss:  0.19613820797089837  | Validation Loss:  0.20247411348763303\n",
      "Epoch:  25  Training Loss:  0.19463133220775344  | Validation Loss:  0.20248682435447649\n",
      "Epoch:  26  Training Loss:  0.19325249377544002  | Validation Loss:  0.20147849954444275\n",
      "Epoch:  27  Training Loss:  0.19186426023848877  | Validation Loss:  0.20078293009269874\n",
      "Epoch:  28  Training Loss:  0.19045367197065793  | Validation Loss:  0.2009743660159365\n",
      "Epoch:  29  Training Loss:  0.18905632580903098  | Validation Loss:  0.20004259993338727\n",
      "Epoch:  30  Training Loss:  0.18766539345701594  | Validation Loss:  0.20014075732090064\n",
      "Epoch:  31  Training Loss:  0.18635079618903183  | Validation Loss:  0.19885199499377132\n",
      "Epoch:  32  Training Loss:  0.18498688683821576  | Validation Loss:  0.19895172551186127\n",
      "Epoch:  33  Training Loss:  0.18368986071838844  | Validation Loss:  0.1989113670834423\n",
      "Epoch:  34  Training Loss:  0.18243921569162458  | Validation Loss:  0.19754377809854654\n",
      "Epoch:  35  Training Loss:  0.18126677979686107  | Validation Loss:  0.19933971621581084\n",
      "Epoch:  36  Training Loss:  0.17997909471237641  | Validation Loss:  0.19623092297442565\n",
      "Epoch:  37  Training Loss:  0.17878230952313184  | Validation Loss:  0.19567779068058058\n",
      "Epoch:  38  Training Loss:  0.17765919846464584  | Validation Loss:  0.19607307642874633\n",
      "Epoch:  39  Training Loss:  0.17641154223884198  | Validation Loss:  0.19795354786356525\n",
      "Epoch:  40  Training Loss:  0.1752100458249649  | Validation Loss:  0.19227207712167818\n",
      "Epoch:  41  Training Loss:  0.17422928505011157  | Validation Loss:  0.19303711305355883\n",
      "Epoch:  42  Training Loss:  0.17318167970315132  | Validation Loss:  0.1926501911331916\n",
      "Epoch:  43  Training Loss:  0.17212141713717008  | Validation Loss:  0.1879921969930096\n",
      "Epoch:  44  Training Loss:  0.17112748041461162  | Validation Loss:  0.18899871289906417\n",
      "Epoch:  45  Training Loss:  0.17006951511836157  | Validation Loss:  0.18772198859403824\n",
      "Epoch:  46  Training Loss:  0.169151671230793  | Validation Loss:  0.18562369727524075\n",
      "Epoch:  47  Training Loss:  0.16826383672090023  | Validation Loss:  0.1854104794253259\n",
      "Epoch:  48  Training Loss:  0.16732050081020303  | Validation Loss:  0.18496426027378388\n",
      "Epoch:  49  Training Loss:  0.16633407442787604  | Validation Loss:  0.18279782206525463\n",
      "Epoch:  50  Training Loss:  0.1654548767461465  | Validation Loss:  0.1817428407937112\n",
      "Epoch:  51  Training Loss:  0.16452385706586256  | Validation Loss:  0.18091241402562552\n",
      "Epoch:  52  Training Loss:  0.16385546579049212  | Validation Loss:  0.18158865539279914\n",
      "Epoch:  53  Training Loss:  0.16294481469121766  | Validation Loss:  0.17988093937995167\n",
      "Epoch:  54  Training Loss:  0.16232707608286373  | Validation Loss:  0.1774031257576491\n",
      "Epoch:  55  Training Loss:  0.16146504292250743  | Validation Loss:  0.17710205586351588\n",
      "Epoch:  56  Training Loss:  0.16072840080838677  | Validation Loss:  0.17672002972406747\n",
      "Epoch:  57  Training Loss:  0.15995640436084982  | Validation Loss:  0.17520198666837794\n",
      "Epoch:  58  Training Loss:  0.15920214085973922  | Validation Loss:  0.17521389661985037\n",
      "Epoch:  59  Training Loss:  0.15858749604685698  | Validation Loss:  0.17353249927596934\n",
      "Epoch:  60  Training Loss:  0.15780590522209567  | Validation Loss:  0.17329143309557932\n",
      "Epoch:  61  Training Loss:  0.15718082086159604  | Validation Loss:  0.17261627382604328\n",
      "Epoch:  62  Training Loss:  0.15653380996272256  | Validation Loss:  0.1718587978528096\n",
      "Epoch:  63  Training Loss:  0.15595720057413093  | Validation Loss:  0.17291395939313448\n",
      "Epoch:  64  Training Loss:  0.15533321587725876  | Validation Loss:  0.17062679043359305\n",
      "Epoch:  65  Training Loss:  0.15474467988044333  | Validation Loss:  0.168826917836652\n",
      "Epoch:  66  Training Loss:  0.15406264409002188  | Validation Loss:  0.17089962139284823\n",
      "Epoch:  67  Training Loss:  0.15361026032408492  | Validation Loss:  0.16720267376427114\n",
      "Epoch:  68  Training Loss:  0.1528789552612255  | Validation Loss:  0.168223075568676\n",
      "Epoch:  69  Training Loss:  0.15239874901247238  | Validation Loss:  0.16938415588535502\n",
      "Epoch:  70  Training Loss:  0.15190916226133372  | Validation Loss:  0.16890646530502645\n",
      "Epoch:  71  Training Loss:  0.1512841448896353  | Validation Loss:  0.1671140404671607\n",
      "Epoch:  72  Training Loss:  0.15082460192390573  | Validation Loss:  0.16645243040908722\n",
      "Epoch:  73  Training Loss:  0.15040445243621475  | Validation Loss:  0.1662735039665854\n",
      "Epoch:  74  Training Loss:  0.14989115451677062  | Validation Loss:  0.16635832618150484\n",
      "Epoch:  75  Training Loss:  0.14945284958048702  | Validation Loss:  0.16486638987205438\n",
      "Epoch:  76  Training Loss:  0.148940286102302  | Validation Loss:  0.16546684349253332\n",
      "Epoch:  77  Training Loss:  0.1484533511378967  | Validation Loss:  0.16613037249393012\n",
      "Epoch:  78  Training Loss:  0.1481070584696434  | Validation Loss:  0.1646119502521831\n",
      "Epoch:  79  Training Loss:  0.14761574544222889  | Validation Loss:  0.1657254131147142\n",
      "Epoch:  80  Training Loss:  0.14727692037023726  | Validation Loss:  0.16312646451433735\n",
      "Epoch:  81  Training Loss:  0.14674418820358845  | Validation Loss:  0.1633347919177727\n",
      "Epoch:  82  Training Loss:  0.14633232584281272  | Validation Loss:  0.16280579121684183\n",
      "Epoch:  83  Training Loss:  0.14584361332265264  | Validation Loss:  0.1626221829882035\n",
      "Epoch:  84  Training Loss:  0.14551532517614904  | Validation Loss:  0.16264294127564458\n",
      "Epoch:  85  Training Loss:  0.145061146196692  | Validation Loss:  0.16177042992510035\n",
      "Epoch:  86  Training Loss:  0.14474102031068675  | Validation Loss:  0.16206233084378158\n",
      "Epoch:  87  Training Loss:  0.14438540899204463  | Validation Loss:  0.16320427925981715\n",
      "Epoch:  88  Training Loss:  0.1440483092774608  | Validation Loss:  0.1620994935021598\n",
      "Epoch:  89  Training Loss:  0.14363902144100832  | Validation Loss:  0.16236029477161767\n",
      "Epoch:  90  Training Loss:  0.14329382067582228  | Validation Loss:  0.160779069954827\n",
      "Epoch:  91  Training Loss:  0.14295469579822417  | Validation Loss:  0.16066043669655478\n",
      "Epoch:  92  Training Loss:  0.14251002680405112  | Validation Loss:  0.1614131668262933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  93  Training Loss:  0.14217944086749346  | Validation Loss:  0.16105896688600968\n",
      "Epoch:  94  Training Loss:  0.14188917657639968  | Validation Loss:  0.16029452757369836\n",
      "Epoch:  95  Training Loss:  0.1416239685546413  | Validation Loss:  0.15974805155802055\n",
      "Epoch:  96  Training Loss:  0.14126552657997804  | Validation Loss:  0.1625413619114097\n",
      "Epoch:  97  Training Loss:  0.14087379014155485  | Validation Loss:  0.15991073538213085\n",
      "Epoch:  98  Training Loss:  0.14062563159446306  | Validation Loss:  0.15996853288637816\n",
      "Epoch:  99  Training Loss:  0.14036240254876164  | Validation Loss:  0.15929422611315575\n"
     ]
    }
   ],
   "source": [
    "def SGD(epochs=10, hidden_dim = 30):\n",
    "    model = GRU(hidden_dim, emb_dim=300)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    criterion = nn.BCEWithLogitsLoss() # reduction='none'\n",
    "    # nn.MultiLabelSoftMarginLoss() nn.BCELoss()\n",
    "\n",
    "    # pre-trained Glove.\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tr_loss, val_loss = 0.0, 0.0\n",
    "        \n",
    "        # training \n",
    "        model.train() \n",
    "        for x, y in train_dl: \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x).squeeze()\n",
    "            loss = criterion(preds, y) # loss = (loss * weight).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        tr_loss /= len(train_dl)\n",
    "\n",
    "        # testing\n",
    "        model.eval()\n",
    "        target_list, preds_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in valid_dl:\n",
    "                preds = model(x).squeeze()\n",
    "                loss = criterion(preds, y) # loss = (loss * weight).mean()\n",
    "                preds = (1/(1 + np.exp(-preds))).numpy() # preds.numpy()\n",
    "                preds_list.extend(preds)\n",
    "                target_list.extend(y.numpy())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(valid_dl)\n",
    "\n",
    "        print('Epoch: ',epoch,' Training Loss: ',tr_loss,' | Validation Loss: ',val_loss)\n",
    "        \n",
    "    return target_list, preds_list\n",
    "\n",
    "target_list, preds_list = SGD(100, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.4452084292337929\n"
     ]
    }
   ],
   "source": [
    "preds_te_arr = np.array(preds_list).reshape((-1, 25))\n",
    "\n",
    "preds_te_arr[preds_te_arr >= 0.5] = 1\n",
    "preds_te_arr[preds_te_arr < 0.5] = 0\n",
    "\n",
    "te_y = np.array(target_list).reshape((-1, 25))\n",
    "\n",
    "print('F1-Score:', f1_score(te_y,preds_te_arr,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tutorial:\n",
    "\n",
    "* data processing in pytorch: http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "\n",
    "* imbalance dataset in pytorch: https://discuss.pytorch.org/t/multi-label-multi-class-class-imbalance/37573/9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
