{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch   \n",
    "from torchtext import data \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "import os\n",
    "from torchtext.vocab import Vectors # downloaded word embedding\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from ast import literal_eval # convert string to list\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import re\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset and testset.\n",
    "dataset = pd.read_csv(\"pre-processed data/new_label_dataset.csv\")[['content', 'categories']]\n",
    "testset = pd.read_csv(\"pre-processed data/new_label_testset.csv\")[['content', 'categories']]\n",
    "\n",
    "# convert string to list.\n",
    "dataset['categories'] = dataset['categories'].apply(lambda x:literal_eval(x))\n",
    "testset['categories'] = testset['categories'].apply(lambda x:literal_eval(x))\n",
    "\n",
    "# seperate 'categories' column.\n",
    "cat_tr_list = []\n",
    "for i in dataset['categories']:\n",
    "    cat_tr_list.append(i)\n",
    "    \n",
    "cat_te_list = []\n",
    "for i in testset['categories']:\n",
    "    cat_te_list.append(i)\n",
    "\n",
    "mlb_tr = MultiLabelBinarizer()\n",
    "labels_tr = mlb_tr.fit_transform(cat_tr_list)\n",
    "categories_tr = mlb_tr.classes_\n",
    "\n",
    "mlb_te = MultiLabelBinarizer()\n",
    "labels_te = mlb_te.fit_transform(cat_te_list)\n",
    "categories_te = mlb_te.classes_\n",
    "\n",
    "# convert list to columns.\n",
    "for i, cat in enumerate(categories_tr):\n",
    "    dataset[cat] = labels_tr[:, i]\n",
    "    \n",
    "\n",
    "for i, cat in enumerate(categories_te):\n",
    "    testset[cat] = labels_te[:, i]\n",
    "    \n",
    "    \n",
    "dataset.to_csv(\"pre-processed data/new_label_dataset_1.csv\")\n",
    "testset.to_csv(\"pre-processed data/new_label_testset_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29114, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0213, 0.0016, 0.0224, 0.0324, 0.0110, 0.0178, 0.0798, 0.0559, 0.0019,\n",
       "        0.0807, 0.0026, 0.0844, 0.0742, 0.0022, 0.1038, 0.0089, 0.1129, 0.0180,\n",
       "        0.0316, 0.0032, 0.0994, 0.0169, 0.0811, 0.0019, 0.0338],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for imbalance dataset. reduction = 'none' in loss function.\n",
    "weight = torch.tensor(labels_tr.sum(axis=0)/sum(labels_tr.sum(axis=0)))\n",
    "\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset = pd.read_csv(\"pre-processed data/label_feature_dataset.csv\")\n",
    "features_testset = pd.read_csv(\"pre-processed data/label_feature_testset.csv\")\n",
    "\n",
    "col = list(features_dataset.columns)[:-2]\n",
    "\n",
    "tr_X, te_X = features_dataset[col].values, features_testset[col].values\n",
    "\n",
    "ml_weight = np.ones((1, 14))\n",
    "ml_weight[0, :4] = 1.5\n",
    "\n",
    "# range to 0 and 1.\n",
    "tr_X = preprocessing.scale(tr_X)*ml_weight\n",
    "te_X = preprocessing.scale(te_X)*ml_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29114, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2020)\n",
    "\n",
    "# the same as system_2.\n",
    "TEXT = data.Field(sequential=True, tokenize=lambda x: x.split(), lower=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "fields = [(v, LABEL) for v in categories_tr]\n",
    "\n",
    "fields = [(None, None), ('content', TEXT), (None, None)] + fields\n",
    "\n",
    "dataset, testset = data.TabularDataset.splits(\n",
    "    path = 'pre-processed data/',\n",
    "    train = 'new_label_dataset_1.csv',\n",
    "    test = 'new_label_testset_1.csv',\n",
    "    format = 'csv',\n",
    "    fields = fields,\n",
    "    skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 6392\n",
      "Top words:  [('earthquake', 2846), ('school', 2577), ('shoot', 2453), ('nepal', 2414), ('philippine', 2358)]\n"
     ]
    }
   ],
   "source": [
    "# tr_X, te_X = dataset.split(split_ratio=0.8, random_state = random.seed(2020))\n",
    "# tr_x, val_x = tr_X.split(split_ratio=0.7, random_state = random.seed(2020))\n",
    "\n",
    "# load downloaded glove word embedding.\n",
    "cache = '.vector_cache'\n",
    "if not os.path.exists(cache): os.mkdir(cache)\n",
    "vectors = Vectors(name='./glove.840B.300d.txt', cache=cache)\n",
    "\n",
    "# create vocab.\n",
    "TEXT.build_vocab(dataset, min_freq=3, vectors=vectors)\n",
    "\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "print(\"Top words: \", TEXT.vocab.freqs.most_common(5))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((dataset, testset), \n",
    "                                                            batch_sizes = (1, 1),\n",
    "                                                            sort=False,\n",
    "                                                            sort_key=None,\n",
    "                                                            shuffle=False,\n",
    "                                                            sort_within_batch=False,\n",
    "                                                            repeat=False,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars, vector):\n",
    "            self.dl, self.x_var, self.y_vars, self.vector = dl, x_var, y_vars, vector # x_var is content, y_var is label.\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, batch in enumerate(self.dl):\n",
    "            x = getattr(batch, self.x_var) # return value of x_var \"content\" in object batch.\n",
    "            y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            z = self.vector[i*len(batch):i*len(batch)+len(batch), :]\n",
    "            yield (x, y, z)\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "train_dl = BatchWrapper(train_iterator, \"content\", list(categories_tr), tr_X)\n",
    "valid_dl = BatchWrapper(valid_iterator, \"content\", list(categories_tr), te_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ElementWiseLinear(nn.Module):\n",
    "    __constants__ = ['n_features']\n",
    "    n_features: int\n",
    "    weight: torch.Tensor\n",
    "    def __init__(self, n_features: int, bias: bool = True) -> None:\n",
    "        super(ElementWiseLinear, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(n_features, 1))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(n_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        output = torch.mul(input, self.weight)\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        return output\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.n_features, self.n_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, hidden_dim=30, emb_dim=300):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=1, dropout=0.2, bidirectional = True)\n",
    "        self.customfc = ElementWiseLinear(14, bias=False)\n",
    "        self.fc = nn.Linear(hidden_dim+14, 64)\n",
    "        self.fc1 = nn.Linear(64, 25)\n",
    "\n",
    "    def forward(self, seq, data):\n",
    "        embed = self.embedding(seq)\n",
    "        _, hidden = self.gru(embed)\n",
    "        embed1 = (hidden[-2,:,:]+hidden[-1,:,:])/2.0\n",
    "        embed1 = embed1.T\n",
    "        tensor_data = torch.tensor(data).type(torch.FloatTensor).T\n",
    "        tensor_data_1 = self.customfc(tensor_data)\n",
    "        vector = torch.cat((embed1, tensor_data_1)).T\n",
    "        output = self.fc(vector)\n",
    "        output1 = self.fc1(F.relu(output))\n",
    "        return output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(epochs=10, hidden_dim = 30):\n",
    "    \n",
    "    tr_loss_list, te_loss_list = [], []\n",
    "    \n",
    "    model = GRU(hidden_dim, emb_dim=300)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.3, weight_decay=1e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    # pre-trained Glove.\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tr_loss, val_loss = 0.0, 0.0\n",
    "        \n",
    "        # training \n",
    "        model.train() \n",
    "        for x, y, z in train_dl: \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x, z)\n",
    "            loss = criterion(preds, y)\n",
    "            loss = (loss * weight).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_loss += loss.item()*7\n",
    "\n",
    "        tr_loss /= len(train_dl)\n",
    "\n",
    "        # testing\n",
    "        model.eval()\n",
    "        target_list, preds_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, z in valid_dl:\n",
    "                preds = model(x, z)\n",
    "                loss = criterion(preds, y)\n",
    "                loss = (loss * weight).mean()\n",
    "                preds = (1/(1 + np.exp(-preds))).numpy() # preds.numpy()\n",
    "                preds_list.extend(preds)\n",
    "                target_list.extend(y.numpy())\n",
    "                val_loss += loss.item()*7\n",
    "\n",
    "        val_loss /= len(valid_dl)\n",
    "        \n",
    "        tr_loss_list.append(tr_loss)\n",
    "        te_loss_list.append(val_loss)\n",
    "\n",
    "        print('Epoch: ',epoch,' Training Loss: ',tr_loss,' | Validation Loss: ',val_loss)\n",
    "        \n",
    "        if len(te_loss_list) > 2:\n",
    "            if (te_loss_list[-2] - te_loss_list[-1]) < 0.000001: #(te_loss_list[-2] - te_loss_list[-1]) > 0 and \n",
    "                break\n",
    "        \n",
    "    return target_list, preds_list, tr_loss_list, te_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  0.09346826095240827  | Validation Loss:  0.177011763406562\n",
      "Epoch:  1  Training Loss:  0.0832686817530013  | Validation Loss:  0.1723366556954481\n",
      "Epoch:  2  Training Loss:  0.08174097439472003  | Validation Loss:  0.1666514802469139\n",
      "Epoch:  3  Training Loss:  0.08067711230065715  | Validation Loss:  0.16325217443257656\n",
      "Epoch:  4  Training Loss:  0.07998985743192746  | Validation Loss:  0.16110124012316776\n",
      "Epoch:  5  Training Loss:  0.07951295707458224  | Validation Loss:  0.15926278687191495\n",
      "Epoch:  6  Training Loss:  0.07910366818347207  | Validation Loss:  0.15829222160455209\n",
      "Epoch:  7  Training Loss:  0.07869473523529046  | Validation Loss:  0.15789359003054268\n",
      "Epoch:  8  Training Loss:  0.07830653306528583  | Validation Loss:  0.15764020541516185\n",
      "Epoch:  9  Training Loss:  0.07793431242497802  | Validation Loss:  0.15732502531785245\n",
      "Epoch:  10  Training Loss:  0.07758989372660925  | Validation Loss:  0.1571633225405234\n"
     ]
    }
   ],
   "source": [
    "target_list, preds_list, tr_loss, te_loss = SGD(11, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = [i for i in range(len(tr_loss))]\n",
    "\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.plot(epoch, tr_loss, label='train')\n",
    "# plt.plot(epoch, te_loss, label='test')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_te_arr = np.array(preds_list).reshape((-1, 25))\n",
    "\n",
    "preds_te_arr[preds_te_arr >= 0.5] = 1\n",
    "preds_te_arr[preds_te_arr < 0.5] = 0\n",
    "\n",
    "te_y = np.array(target_list).reshape((-1, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average F1-score:  0.4950096476241462\n",
      "Average accuracy:  0.8851302115173004\n"
     ]
    }
   ],
   "source": [
    "preds_te_arr.shape\n",
    "\n",
    "score = 0\n",
    "acc = 0\n",
    "\n",
    "for i in range(preds_te_arr.shape[1]):\n",
    "    score += f1_score(te_y[:, i], preds_te_arr[:, i],average='macro')\n",
    "    acc += accuracy_score(te_y[:, i], preds_te_arr[:, i])\n",
    "    \n",
    "print(\"Macro-average F1-score: \", score/25)\n",
    "print(\"Average accuracy: \", acc/25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
