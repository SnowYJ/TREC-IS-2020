{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-processed dataset.\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch   \n",
    "from torchtext import data \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "import os\n",
    "from torchtext.vocab import Vectors\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import preprocessing\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset = pd.read_csv(\"pre-processed data/new_label_testset.csv\")[['priority', 'content']]\n",
    "dataset = pd.read_csv(\"pre-processed data/new_label_dataset.csv\")[['priority', 'content']]\n",
    "\n",
    "dataset = dataset[dataset['priority'] != 'Unknown'].reset_index(drop=True)\n",
    "testset = testset[testset['priority'] != 'Unknown'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset = pd.read_csv(\"pre-processed data/label_feature_dataset.csv\")\n",
    "features_testset = pd.read_csv(\"pre-processed data/label_feature_testset.csv\")\n",
    "\n",
    "features_dataset = features_dataset[features_dataset['priority'] != 'Unknown'].reset_index(drop=True)\n",
    "features_testset = features_testset[features_testset['priority'] != 'Unknown'].reset_index(drop=True)\n",
    "\n",
    "print(features_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = list(features_dataset.columns)[:-2]\n",
    "\n",
    "tr_X, te_X = features_dataset[col].values, features_testset[col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range to 0 and 1.\n",
    "\n",
    "tr_X = preprocessing.scale(tr_X)\n",
    "te_X = preprocessing.scale(te_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.ones((1, 14))\n",
    "weight[0, :4] = 1.5\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_X #*= weight\n",
    "# te_X #*= weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2020)\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=lambda x: x.split(), lower=True)\n",
    "LABEL = data.LabelField(dtype = torch.float, batch_first=True)\n",
    "\n",
    "fields = [('priority', LABEL), ('content', TEXT)] \n",
    "\n",
    "dataset, testset = data.TabularDataset.splits(\n",
    "    path = 'pre-processed data/',\n",
    "    train = 'dataset_priority.csv',\n",
    "    test = 'testset_priority.csv',\n",
    "    format = 'csv',\n",
    "    fields = fields,\n",
    "    skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load downloaded glove word embedding.\n",
    "cache = '.vector_cache'\n",
    "if not os.path.exists(cache): os.mkdir(cache)\n",
    "vectors = Vectors(name='./glove.840B.300d.txt', cache=cache)\n",
    "\n",
    "# create vocab.\n",
    "TEXT.build_vocab(dataset, min_freq=3, vectors=vectors)\n",
    "LABEL.build_vocab(dataset)\n",
    "\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "print(\"Top words: \", TEXT.vocab.freqs.most_common(5))  \n",
    "\n",
    "# Word dictionary.\n",
    "print(\"LABEL vocabulary: \", LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "# sort_key = lambda x: len(x.content),sort_within_batch=False, Bucket\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((dataset, testset), \n",
    "                                                            batch_sizes = (1, 1), \n",
    "                                                            sort=False,\n",
    "                                                            sort_key=None,\n",
    "                                                            shuffle=False,\n",
    "                                                            sort_within_batch=False,\n",
    "                                                            repeat=False,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, vector):\n",
    "            self.dl, self.vector = dl, vector # x_var is content, y_var is label.\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, batch in enumerate(self.dl):\n",
    "            x = batch.content\n",
    "            y = batch.priority\n",
    "            z = self.vector[i*len(batch):i*len(batch)+len(batch), :]\n",
    "            yield (x, y, z)\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "train_dl = BatchWrapper(train_iterator, tr_X)\n",
    "valid_dl = BatchWrapper(valid_iterator, te_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ElementWiseLinear(nn.Module):\n",
    "    __constants__ = ['n_features']\n",
    "    n_features: int\n",
    "    weight: torch.Tensor\n",
    "    def __init__(self, n_features: int, bias: bool = True) -> None:\n",
    "        super(ElementWiseLinear, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(n_features, 1))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(n_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        output = torch.mul(input, self.weight)\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        return output\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.n_features, self.n_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, hidden_dim=30, emb_dim=300):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=1, dropout=0.2, bidirectional = True)\n",
    "        self.customfc = ElementWiseLinear(14, bias=False)\n",
    "        self.fc = nn.Linear(hidden_dim + 14, 128)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, seq, data):\n",
    "        embed = self.embedding(seq)\n",
    "        _, hidden = self.gru(embed)\n",
    "        embed1 = (hidden[-2,:,:]+hidden[-1,:,:])/2.0\n",
    "        embed1 = embed1.T\n",
    "        tensor_data = torch.tensor(data).type(torch.FloatTensor).T\n",
    "        tensor_data_1 = self.customfc(tensor_data)\n",
    "        vector = torch.cat((embed1, tensor_data_1))\n",
    "        output = self.fc(vector.T)\n",
    "        output1 = self.fc1(F.relu(output))\n",
    "        output2 = self.fc2(F.relu(output1))\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(epochs=10, hidden_dim = 30, lr=0.001):\n",
    "    \n",
    "    tr_loss_list, te_loss_list = [], []\n",
    "    \n",
    "    model = GRU(hidden_dim, emb_dim=300)\n",
    "    \n",
    "    weight = torch.tensor([0.7220, 0.1742, 0.0941, 0.0348])\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    # pre-trained Glove.\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tr_loss, val_loss = 0.0, 0.0\n",
    "        \n",
    "        # training \n",
    "        model.train() \n",
    "        for x, y, z in train_dl: \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x, z)#.squeeze()\n",
    "            loss = criterion(preds, y.type(torch.LongTensor))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        tr_loss /= len(train_dl)\n",
    "        tr_loss *= 7\n",
    "        # testing\n",
    "        model.eval()\n",
    "        target_list, preds_list, probs_list = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, z in valid_dl:\n",
    "                preds = model(x, z)#.squeeze()\n",
    "                loss = criterion(preds, y.type(torch.LongTensor)) \n",
    "                probs = F.softmax(preds).detach().numpy()\n",
    "                \n",
    "                probs_list.extend(probs)\n",
    "                preds_list.extend(preds)\n",
    "                target_list.extend(y.numpy())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(valid_dl)\n",
    "        val_loss *= 7\n",
    "        \n",
    "        tr_loss_list.append(tr_loss)\n",
    "        te_loss_list.append(val_loss)\n",
    "\n",
    "        print('Epoch: ',epoch,' Training Loss: ',tr_loss,' | Validation Loss: ',val_loss)\n",
    "        \n",
    "        if len(te_loss_list) > 2:\n",
    "            if (te_loss_list[-2] - te_loss_list[-1]) < 0.00001: # (te_loss_list[-2] - te_loss_list[-1]) > 0 and \n",
    "                break\n",
    "        \n",
    "    return tr_loss_list, te_loss_list, preds_list, target_list, probs_list, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss, valid_loss, preds_list, label_list, prob_list, model = SGD(epochs=20, lr=0.0005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE all:  0.04910742681724399\n"
     ]
    }
   ],
   "source": [
    "# RMSE\n",
    "score = 0\n",
    "\n",
    "for i in range(len(label_list)):\n",
    "    index = int(label_list[i])\n",
    "    if index == 0: weight = 0.25\n",
    "    elif index == 1: weight = 0.5\n",
    "    elif index == 2: weight = 0.75\n",
    "    elif index == 3: weight = 1\n",
    "    else: weight = 0\n",
    "    score += (weight - weight*prob_list[i][index])**2\n",
    "    \n",
    "print(\"RMSE all: \", score/len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = [i for i in range(len(train_loss))]\n",
    "\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.plot(epoch, train_loss, label='train')\n",
    "# plt.plot(epoch, valid_loss, label='test')\n",
    "# plt.title(\"testing process\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inf = dict([i for i in model.named_parameters()])\n",
    "\n",
    "# inf_list = [i.item() for i in inf['customfc.weight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
