{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-class tweets classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. pytorch - LSTM introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/pytorch_LSTM_1.png\" width=\"550\">\n",
    "\n",
    "### LSTM architecture\n",
    "> **model = nn.LSTM(input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional)**\n",
    "1. input_size: 每个单词的 embedding or one-hot encoding 的尺寸，例如：glove 300d。\n",
    "2. hidden_size: 隐含层神经元的个数，例如上图的n。\n",
    "3. num_layers: LSTM的层数，例如上图的层数为1。\n",
    "4. bias: default true。\n",
    "5. batch_first: \n",
    "6. dropout: 0-1\n",
    "7. bidirectional: true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 100])\n",
      "torch.Size([12, 3])\n",
      "torch.Size([12])\n",
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "lstm = nn.LSTM(100, 3, 2)  # word embedding size is 300, hidden layer size is 3, two LSTM layer. \n",
    "print(lstm.all_weights[0][0].shape)\n",
    "print(lstm.all_weights[0][1].shape)\n",
    "print(lstm.all_weights[0][2].shape)\n",
    "print(lstm.all_weights[0][3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM input and output\n",
    "><img src=\"image/pytorch_LSTM.png\" width=\"500\">\n",
    ">\n",
    ">**output, ($\\text{h}_n$, $\\text{c}_n$) = model(input, ($\\text{h}_0$, $\\text{c}_0$))**\n",
    ">\n",
    ">* input维度：(句子长度, batch_size（一次多少个句子）, input_size（每个单词向量的长度）) 。\n",
    ">* $\\text{h}_0, \\text{c}_0$维度：(num_layers * num_directions, batch, hidden_size)。例如，上图有w+1层，单向，hidden_size为每个LSTM结构内隐含层神经元个数。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. sentence length:  5  word embedding: torch.Size([1, 100])\n",
      "2. input is every word in sentence:  torch.Size([1, 1, 100])\n",
      "3. output size:  torch.Size([1, 1, 3])\n",
      "4. h_1 size:  torch.Size([2, 1, 3])\n",
      "5. c_1 size:  torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.randn(1, 100) for _ in range(5)] # In a sentence, 5 words, every words' length is 4.\n",
    "\n",
    "hidden = (torch.randn(2*1, 1, 3), torch.randn(2*1, 1, 3)) # h_0, c_0.\n",
    "\n",
    "for i in inputs:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "print(\"1. sentence length: \",len(inputs),\" word embedding:\",inputs[0].shape)\n",
    "print(\"2. input is every word in sentence: \", i.view(1, 1, -1).shape)\n",
    "print(\"3. output size: \", out.shape)\n",
    "print(\"4. h_1 size: \", hidden[0].shape)\n",
    "print(\"5. c_1 size: \", hidden[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    ">**module(data) = module.forward(data)**\n",
    ">\n",
    "><img src=\"image/forward.png\" width=\"550\">\n",
    "\n",
    "### Handle input with variable length\n",
    ">* torch.nn.utils.rnn.pad_sequence()：把不等长的tensor数据, 补充成等长的tensor数据.\n",
    "* torch.nn.utils.rnn.pack_padded_sequence()：把等长的tensor根据所输入的参数压缩成实际的数据, 同时数据格式变成PackedSequence。\n",
    "* torch.nn.utils.rnn.pad_packed_sequence()：把上面所压缩成PackedSequence的数据还原成tensor类型, 并补成等长的数据。\n",
    "\n",
    "https://blog.csdn.net/kejizuiqianfang/article/details/100835528\n",
    "\n",
    "### Pytorch loss function for classification\n",
    ">\n",
    ">* nn.Softmax and nn.LogSoftmax: \n",
    "* nn.NLLLoss: negative log likelihood loss. \n",
    "* nn.CrossEntropy: the combination of nn.Softmax and nn.NLLLoss.\n",
    "* nn.BCELoss: *bianry* classification loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-processed dataset.\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch   \n",
    "from torchtext import data \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "import os\n",
    "from torchtext.vocab import Vectors\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset processing in pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.read_csv(\"pre-processed data/new_label_testset.csv\")[['priority', 'content']]\n",
    "dataset = pd.read_csv(\"pre-processed data/new_label_dataset.csv\")[['priority', 'content']]\n",
    "\n",
    "dataset = dataset[dataset['priority'] != 'Unknown']\n",
    "\n",
    "testset.to_csv(\"pre-processed data/testset_priority.csv\", index=False)\n",
    "dataset.to_csv(\"pre-processed data/dataset_priority.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'priority': 'Low', 'content': ['philippine', 'flood', 'worsen', 'death', 'toll', 'hit', 'wake', 'gener']}\n",
      "{'priority': 'High', 'content': ['view', 'shell', 'gregoire']}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2020)\n",
    "\n",
    "# loading custom dataset\n",
    "TEXT = data.Field(tokenize=lambda x: x.split() ,batch_first=True, include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float, batch_first=True)\n",
    "\n",
    "fields = [('priority', LABEL), ('content', TEXT)]\n",
    "\n",
    "dataset, testset = data.TabularDataset.splits(\n",
    "    path = 'pre-processed data/',\n",
    "    train = 'dataset_priority.csv',\n",
    "    test = 'testset_priority.csv',\n",
    "    format = 'csv',\n",
    "    fields = fields,\n",
    "    skip_header = True\n",
    ")\n",
    "\n",
    "print(vars(dataset.examples[0]))\n",
    "print(vars(testset.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 6392\n",
      "Size of LABEL vocabulary: 4\n",
      "Top words:  [('earthquake', 2846), ('school', 2577), ('shoot', 2453), ('nepal', 2414), ('philippine', 2358)]\n",
      "LABEL vocabulary:  defaultdict(None, {'Low': 0, 'Medium': 1, 'High': 2, 'Critical': 3})\n"
     ]
    }
   ],
   "source": [
    "# load downloaded glove word embedding.\n",
    "cache = '.vector_cache'\n",
    "if not os.path.exists(cache): os.mkdir(cache)\n",
    "vectors = Vectors(name='./glove.840B.300d.txt', cache=cache)\n",
    "\n",
    "# create vocab.\n",
    "TEXT.build_vocab(dataset, min_freq=3, vectors=vectors)\n",
    "LABEL.build_vocab(dataset)\n",
    "\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "print(\"Top words: \", TEXT.vocab.freqs.most_common(5))  \n",
    "\n",
    "# Word dictionary.\n",
    "print(\"LABEL vocabulary: \", LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight for imbalance dataset.\n",
    "\n",
    "weight = torch.tensor([0.7220, 0.1742, 0.0941, 0.0348])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classifier(nn.Module):\n",
    "    #define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # bi-directional LSTM\n",
    "        self.lstm = nn.GRU(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=bidirectional, \n",
    "                            dropout=dropout,\n",
    "                            batch_first=True)\n",
    "\n",
    "        # dense layer\n",
    "        self.fc = nn.Linear(hidden_dim*2, 128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "        \n",
    "        # don't need activation function if using nn.CrossEntropy\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        # text word index.\n",
    "        embedded = self.embedding(text) # shape = [batch, length, word embedding]\n",
    "        \n",
    "        # packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
    "\n",
    "        packed_output, hidden = self.lstm(packed_embedded) # GRU has two outputs, LSTM has three. eg. packed_output, (hidden, cell)\n",
    "        \n",
    "        # hidden shape [2, 64, num_hidden_nodes] if LSTM shape [1, 64, num_hidden_nodes]\n",
    "        # concat the final forward and backward hidden state (use hidden state, not output)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        hidden_outputs = self.fc(hidden)\n",
    "        \n",
    "        outputs1 = self.fc1(F.relu(hidden_outputs)) # F.log_softmax()\n",
    "        \n",
    "        outputs2 = self.fc2(F.relu(outputs1))\n",
    "        \n",
    "        return outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    #initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        #retrieve text and no. of words\n",
    "        cont, cont_lengths = batch.content \n",
    "\n",
    "        # forward\n",
    "        predictions = model(cont, cont_lengths).squeeze()  \n",
    "\n",
    "        #compute the loss\n",
    "        loss = criterion(predictions, batch.priority.long())\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()       \n",
    "        \n",
    "        #update the weights\n",
    "        optimizer.step() \n",
    "        \n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()   \n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    label_list, preds_list, prob_list = [], [], []\n",
    "    # initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    # deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            \n",
    "            # retrieve text and length\n",
    "            cont, cont_lengths = batch.content \n",
    "\n",
    "            predictions = model(cont, cont_lengths).squeeze()\n",
    "            \n",
    "            # keep result\n",
    "            preds_list.extend(list(np.array(torch.max(predictions, 1)[1].numpy(), dtype=int)))\n",
    "            label_list.extend(list(np.array(batch.priority.numpy(), dtype=int)))\n",
    "            prob_list.extend(F.softmax(predictions).numpy())\n",
    "\n",
    "            # compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.priority.long())\n",
    "            \n",
    "            # train loss.\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), preds_list, label_list, prob_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.CrossEntropyLoss(weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_vocab = len(TEXT.vocab)\n",
    "\n",
    "\n",
    "def SGD(tr_x, val_x, n_epoches=100, lr = 0.001, dropout = 0.3, batch_size = 32, num_hidden_nodes = 32):\n",
    "    \n",
    "    train_loss_list, valid_loss_list = [], []\n",
    "    \n",
    "    embedding_dim = 300\n",
    "    num_output_nodes = 4\n",
    "    num_layers = 1 # depth\n",
    "    bidirection = True\n",
    "    \n",
    "    # GPU or CPU.\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "    \n",
    "    # Load an iterator\n",
    "    train_iterator, valid_iterator = data.BucketIterator.splits((tr_x, val_x), \n",
    "                                                                batch_size = batch_size,\n",
    "                                                                sort_key = lambda x: len(x.content),\n",
    "                                                                sort_within_batch=True,\n",
    "                                                                device=device)\n",
    "\n",
    "\n",
    "    \n",
    "    # model print(model)\n",
    "    model = RNN_classifier(size_of_vocab, embedding_dim, \n",
    "                       num_hidden_nodes,num_output_nodes, \n",
    "                       num_layers, bidirectional = True, \n",
    "                       dropout = dropout)\n",
    "    \n",
    "    # pre-trained Glove.\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "    \n",
    "    # define optimizer and loss.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-8)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight) # nn.NLLLoss()\n",
    "    \n",
    "    # training\n",
    "    for epoch in range(n_epoches):\n",
    "\n",
    "        #train the model\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion)\n",
    "\n",
    "        #evaluate the model\n",
    "        valid_loss, preds_list, label_list, prob_list = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        \n",
    "        print(f\"Epoch: {epoch:d} : Train Loss: {train_loss:.4f} | Valid loss: {valid_loss:.4f}\")\n",
    "        \n",
    "        if len(valid_loss_list) > 2:\n",
    "            if (valid_loss_list[-2] - valid_loss_list[-1])<0.0001 and (valid_loss_list[-2] - valid_loss_list[-1])>0:\n",
    "                break\n",
    "        \n",
    "    return train_loss_list, valid_loss_list, preds_list, label_list, prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hyper-parameter choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 : Train Loss: 1.2320 | Valid loss: 1.0543\n",
      "Epoch: 1 : Train Loss: 0.9356 | Valid loss: 0.7242\n",
      "Epoch: 2 : Train Loss: 0.6523 | Valid loss: 0.4206\n",
      "Epoch: 3 : Train Loss: 0.4588 | Valid loss: 0.2609\n",
      "Epoch: 4 : Train Loss: 0.3895 | Valid loss: 0.2085\n",
      "Epoch: 5 : Train Loss: 0.3715 | Valid loss: 0.1910\n",
      "Epoch: 6 : Train Loss: 0.3655 | Valid loss: 0.1837\n",
      "Epoch: 7 : Train Loss: 0.3618 | Valid loss: 0.1803\n",
      "Epoch: 8 : Train Loss: 0.3582 | Valid loss: 0.1783\n",
      "Epoch: 9 : Train Loss: 0.3557 | Valid loss: 0.1771\n",
      "Epoch: 10 : Train Loss: 0.3535 | Valid loss: 0.1763\n",
      "Epoch: 11 : Train Loss: 0.3522 | Valid loss: 0.1757\n",
      "Epoch: 12 : Train Loss: 0.3504 | Valid loss: 0.1752\n",
      "Epoch: 13 : Train Loss: 0.3484 | Valid loss: 0.1747\n",
      "Epoch: 14 : Train Loss: 0.3475 | Valid loss: 0.1744\n",
      "Epoch: 15 : Train Loss: 0.3459 | Valid loss: 0.1742\n",
      "Epoch: 16 : Train Loss: 0.3456 | Valid loss: 0.1741\n",
      "Epoch: 17 : Train Loss: 0.3445 | Valid loss: 0.1737\n",
      "Epoch: 18 : Train Loss: 0.3434 | Valid loss: 0.1736\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss, preds_list, label_list, prob_list = SGD(dataset, testset, \n",
    "                             n_epoches=50, \n",
    "                             lr=0.0008, \n",
    "                             dropout=0.2, \n",
    "                             batch_size = 64, \n",
    "                             num_hidden_nodes = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE all:  0.05146055561233332\n"
     ]
    }
   ],
   "source": [
    "# RMSE\n",
    "score = 0\n",
    "\n",
    "for i in range(len(label_list)):\n",
    "    index = label_list[i]\n",
    "    if index == 0: weight = 0.25\n",
    "    elif index == 1: weight = 0.5\n",
    "    elif index == 2: weight = 0.75\n",
    "    elif index == 3: weight = 1\n",
    "    else: weight = 0\n",
    "    score += (weight - weight*prob_list[i][index])**2\n",
    "    \n",
    "print(\"RMSE all: \", score/len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch = [i for i in range(len(train_loss))]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epoch, train_loss, label='train')\n",
    "plt.plot(epoch, valid_loss, label='test')\n",
    "plt.title(\"testing process\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8770020784937034\n",
      "Precision: 0.21925051962342584\n",
      "Recall: 0.25\n",
      "F1-Score: 0.233617769671704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "Y_te = label_list\n",
    "preds_te = preds_list\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    ">1. choose SGD rather than Adam.\n",
    ">2. using nn.CrossEntropyLoss() (nn.logSoftmax() + nn.NLLLoss()) for multi-class classification.\n",
    ">3. weight for imbalance dataset.\n",
    "\n",
    "tutorial: https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
