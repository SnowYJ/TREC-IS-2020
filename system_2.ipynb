{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-class tweets classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. pytorch - LSTM introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/pytorch_LSTM_1.png\" width=\"550\">\n",
    "\n",
    "### 1.1 LSTM architecture\n",
    "> **model = nn.LSTM(input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional)**\n",
    "1. input_size: 每个单词的 embedding or one-hot encoding 的尺寸，例如：glove 300d。\n",
    "2. hidden_size: 隐含层神经元的个数，例如上图的n。\n",
    "3. num_layers: LSTM的层数，例如上图的层数为1。\n",
    "4. bias: default true。\n",
    "5. batch_first: \n",
    "6. dropout: 0-1\n",
    "7. bidirectional: true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 100])\n",
      "torch.Size([12, 3])\n",
      "torch.Size([12])\n",
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "lstm = nn.LSTM(100, 3, 2)  # word embedding size is 300, hidden layer size is 3, two LSTM layer. \n",
    "print(lstm.all_weights[0][0].shape)\n",
    "print(lstm.all_weights[0][1].shape)\n",
    "print(lstm.all_weights[0][2].shape)\n",
    "print(lstm.all_weights[0][3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 LSTM input and output\n",
    "><img src=\"image/pytorch_LSTM.png\" width=\"500\">\n",
    ">\n",
    ">**output, ($\\text{h}_n$, $\\text{c}_n$) = model(input, ($\\text{h}_0$, $\\text{c}_0$))**\n",
    ">\n",
    ">* input维度：(句子长度, batch_size（一次多少个句子）, input_size（每个单词向量的长度）) 。\n",
    ">* $\\text{h}_0, \\text{c}_0$维度：(num_layers * num_directions, batch, hidden_size)。例如，上图有w+1层，单向，hidden_size为每个LSTM结构内隐含层神经元个数。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. sentence length:  5  word embedding: torch.Size([1, 100])\n",
      "2. input is every word in sentence:  torch.Size([1, 1, 100])\n",
      "3. output size:  torch.Size([1, 1, 3])\n",
      "4. h_1 size:  torch.Size([2, 1, 3])\n",
      "5. c_1 size:  torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.randn(1, 100) for _ in range(5)] # In a sentence, 5 words, every words' length is 4.\n",
    "\n",
    "hidden = (torch.randn(2*1, 1, 3), torch.randn(2*1, 1, 3)) # h_0, c_0.\n",
    "\n",
    "for i in inputs:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "print(\"1. sentence length: \",len(inputs),\" word embedding:\",inputs[0].shape)\n",
    "print(\"2. input is every word in sentence: \", i.view(1, 1, -1).shape)\n",
    "print(\"3. output size: \", out.shape)\n",
    "print(\"4. h_1 size: \", hidden[0].shape)\n",
    "print(\"5. c_1 size: \", hidden[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 forward pass\n",
    ">**module(data) = module.forward(data)**\n",
    ">\n",
    "><img src=\"image/forward.png\" width=\"550\">\n",
    "\n",
    "### 1.4 handle input with variable length\n",
    ">* torch.nn.utils.rnn.pad_sequence()：把不等长的tensor数据, 补充成等长的tensor数据.\n",
    "* torch.nn.utils.rnn.pack_padded_sequence()：把等长的tensor根据所输入的参数压缩成实际的数据, 同时数据格式变成PackedSequence。\n",
    "* torch.nn.utils.rnn.pad_packed_sequence()：把上面所压缩成PackedSequence的数据还原成tensor类型, 并补成等长的数据。\n",
    "\n",
    "https://blog.csdn.net/kejizuiqianfang/article/details/100835528\n",
    "\n",
    "### 1.5 pytorch loss function for classification\n",
    ">\n",
    ">* nn.Softmax and nn.LogSoftmax: \n",
    "* nn.NLLLoss: negative log likelihood loss. \n",
    "* nn.CrossEntropy: the combination of nn.Softmax and nn.NLLLoss.\n",
    "* nn.BCELoss: *bianry* classification loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. bi-directional GRU for priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-processed dataset.\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch   \n",
    "from torchtext import data \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "import os\n",
    "from torchtext.vocab import Vectors\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 dataset processing in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>priority</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Medium</td>\n",
       "      <td>pagasa advisory red warning metro manila heavy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>pagasa advisory red warning metro manila heavy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Medium</td>\n",
       "      <td>pagasa advisory red warning metro manila heavy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Medium</td>\n",
       "      <td>pagasa advisory red warning metro manila heavy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Medium</td>\n",
       "      <td>flood alert san marcelino taft ayala manila pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  priority                                            content\n",
       "0   Medium  pagasa advisory red warning metro manila heavy...\n",
       "1   Medium  pagasa advisory red warning metro manila heavy...\n",
       "2   Medium  pagasa advisory red warning metro manila heavy...\n",
       "3   Medium  pagasa advisory red warning metro manila heavy...\n",
       "4   Medium  flood alert san marcelino taft ayala manila pa..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice dataset is extremely unbalanced.\n",
    "dataset = pd.read_csv(\"pre-processed data/dataset_aug_priority.csv\", usecols=['content','priority'])\n",
    "\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'priority': 'Medium', 'content': ['pagasa', 'advisory', 'red', 'warning', 'metro', 'manila', 'heavy', 'intense', 'rain', 'next', 'hr', 'flood', 'low', 'lie', 'area', 'near', 'river']}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2020)\n",
    "\n",
    "# loading custom dataset\n",
    "TEXT = data.Field(tokenize=lambda x: x.split() ,batch_first=True, include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float, batch_first=True)\n",
    "\n",
    "fields = [('priority', LABEL), ('content', TEXT)]\n",
    "\n",
    "dataset=data.TabularDataset(path = 'pre-processed data/dataset_aug_priority.csv',format = 'csv',fields = fields,skip_header = True)\n",
    "\n",
    "print(vars(dataset.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 7994\n",
      "Size of LABEL vocabulary: 4\n",
      "Top words:  [('shoot', 3638), ('flood', 3344), ('school', 3198), ('people', 2834), ('after', 2709)]\n",
      "LABEL vocabulary:  defaultdict(None, {'Medium': 0, 'High': 1, 'Critical': 2, 'Unknown': 3})\n"
     ]
    }
   ],
   "source": [
    "tr_X, te_X = dataset.split(split_ratio=0.8, random_state = random.seed(2020))\n",
    "tr_x, val_x = tr_X.split(split_ratio=0.7, random_state = random.seed(2020))\n",
    "\n",
    "# load downloaded glove word embedding.\n",
    "cache = '.vector_cache'\n",
    "if not os.path.exists(cache): os.mkdir(cache)\n",
    "vectors = Vectors(name='./glove.840B.300d.txt', cache=cache)\n",
    "\n",
    "# create vocab.\n",
    "TEXT.build_vocab(tr_X, min_freq=3, vectors=vectors)\n",
    "LABEL.build_vocab(tr_X)\n",
    "\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "print(\"Top words: \", TEXT.vocab.freqs.most_common(5))  \n",
    "\n",
    "# Word dictionary.\n",
    "print(\"LABEL vocabulary: \", LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Architecture\n",
    "\n",
    "<img src=\"image/architecture.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classifier(nn.Module):\n",
    "    #define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # bi-directional LSTM\n",
    "        self.lstm = nn.GRU(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=bidirectional, \n",
    "                            dropout=dropout,\n",
    "                            batch_first=True)\n",
    "\n",
    "        # dense layer\n",
    "        self.fc = nn.Linear(hidden_dim*2, 64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64, output_dim)\n",
    "\n",
    "        # don't need activation function if using nn.CrossEntropy\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        # text word index.\n",
    "        embedded = self.embedding(text) # shape = [batch, length, word embedding]\n",
    "        \n",
    "        # packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
    "\n",
    "        packed_output, hidden = self.lstm(packed_embedded) # GRU has two outputs, LSTM has three. eg. packed_output, (hidden, cell)\n",
    "        \n",
    "        # hidden shape [2, 64, num_hidden_nodes] if LSTM shape [1, 64, num_hidden_nodes]\n",
    "        # concat the final forward and backward hidden state (use hidden state, not output)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        hidden_outputs = self.fc(hidden)\n",
    "        \n",
    "        dense_outputs = self.fc1(F.relu(hidden_outputs))\n",
    "        \n",
    "        #Final activation function\n",
    "        # outputs=self.act(dense_outputs)\n",
    "        \n",
    "        return dense_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 training and evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    #initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        #retrieve text and no. of words\n",
    "        cont, cont_lengths = batch.content \n",
    "\n",
    "        # forward\n",
    "        predictions = model(cont, cont_lengths).squeeze()  \n",
    "\n",
    "        #compute the loss\n",
    "        loss = criterion(predictions, batch.priority.long())  \n",
    "        \n",
    "        # backward\n",
    "        loss.backward()       \n",
    "        \n",
    "        #update the weights\n",
    "        optimizer.step() \n",
    "        \n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()     \n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    label_list, preds_list = [], []\n",
    "    # initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    # deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            \n",
    "            # retrieve text and length\n",
    "            cont, cont_lengths = batch.content \n",
    "\n",
    "            predictions = model(cont, cont_lengths).squeeze()\n",
    "            \n",
    "            # keep result\n",
    "            preds_list.extend(list(np.array(torch.max(predictions, 1)[1].numpy(), dtype=int)))\n",
    "            label_list.extend(list(np.array(batch.priority.numpy(), dtype=int)))\n",
    "\n",
    "            # compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.priority.long())\n",
    "            \n",
    "            # train loss.\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), preds_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_vocab = len(TEXT.vocab)\n",
    "\n",
    "\n",
    "def SGD(tr_x, val_x, n_epoches=100, lr = 0.001, dropout = 0.3, batch_size = 32, num_hidden_nodes = 32):\n",
    "    \n",
    "    train_loss_list, valid_loss_list = [], []\n",
    "    \n",
    "    embedding_dim = 300\n",
    "    num_output_nodes = 5\n",
    "    num_layers = 1 # depth\n",
    "    bidirection = True\n",
    "    \n",
    "    # GPU or CPU.\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "    \n",
    "    # Load an iterator\n",
    "    train_iterator, valid_iterator = data.BucketIterator.splits((tr_x, val_x), \n",
    "                                                                batch_size = batch_size,\n",
    "                                                                sort_key = lambda x: len(x.content),\n",
    "                                                                sort_within_batch=True,\n",
    "                                                                device=device)\n",
    "\n",
    "\n",
    "\n",
    "    # model print(model)\n",
    "    model = RNN_classifier(size_of_vocab, embedding_dim, \n",
    "                       num_hidden_nodes,num_output_nodes, \n",
    "                       num_layers, bidirectional = True, \n",
    "                       dropout = dropout)\n",
    "    \n",
    "    # pre-trained Glove.\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "    \n",
    "    # define optimizer and loss.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # training\n",
    "    for epoch in range(n_epoches):\n",
    "\n",
    "        #train the model\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion)\n",
    "\n",
    "        #evaluate the model\n",
    "        valid_loss, preds_list, label_list = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        \n",
    "        print(f\"Epoch: {epoch:d} : Train Loss: {train_loss:.4f} | Valid loss: {valid_loss:.4f}\")\n",
    "        \n",
    "        #if (len(valid_loss_list) >= 2):\n",
    "        #    if (valid_loss_list[-2] - valid_loss_list[-1]) < 0.001: break\n",
    "        \n",
    "    return train_loss_list, valid_loss_list, preds_list, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 hyper-parameter choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for hn in [10, 20, 40, 100]:\n",
    "#     for lr in [0.001, 0.01, 0.1]:\n",
    "#         print(\"hidden layer nodes: \", hn, \", learning rate: \", lr)\n",
    "#         train_loss, valid_loss = SGD(tr_x, val_x, \n",
    "#                                      n_epoches=50, lr=lr, \n",
    "#                                      dropout=0, batch_size = 64, \n",
    "#                                      num_hidden_nodes = hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 : Train Loss: 1.2409 | Valid loss: 1.1040\n",
      "Epoch: 1 : Train Loss: 1.0826 | Valid loss: 1.0720\n",
      "Epoch: 2 : Train Loss: 1.0636 | Valid loss: 1.0570\n",
      "Epoch: 3 : Train Loss: 1.0497 | Valid loss: 1.0419\n",
      "Epoch: 4 : Train Loss: 1.0327 | Valid loss: 1.0231\n",
      "Epoch: 5 : Train Loss: 1.0113 | Valid loss: 0.9993\n",
      "Epoch: 6 : Train Loss: 0.9876 | Valid loss: 0.9761\n",
      "Epoch: 7 : Train Loss: 0.9651 | Valid loss: 0.9539\n",
      "Epoch: 8 : Train Loss: 0.9453 | Valid loss: 0.9350\n",
      "Epoch: 9 : Train Loss: 0.9268 | Valid loss: 0.9206\n",
      "Epoch: 10 : Train Loss: 0.9100 | Valid loss: 0.9021\n",
      "Epoch: 11 : Train Loss: 0.8933 | Valid loss: 0.8879\n",
      "Epoch: 12 : Train Loss: 0.8772 | Valid loss: 0.8718\n",
      "Epoch: 13 : Train Loss: 0.8611 | Valid loss: 0.8535\n",
      "Epoch: 14 : Train Loss: 0.8449 | Valid loss: 0.8383\n",
      "Epoch: 15 : Train Loss: 0.8279 | Valid loss: 0.8228\n",
      "Epoch: 16 : Train Loss: 0.8116 | Valid loss: 0.8059\n",
      "Epoch: 17 : Train Loss: 0.7952 | Valid loss: 0.7905\n",
      "Epoch: 18 : Train Loss: 0.7780 | Valid loss: 0.7744\n",
      "Epoch: 19 : Train Loss: 0.7618 | Valid loss: 0.7599\n",
      "Epoch: 20 : Train Loss: 0.7457 | Valid loss: 0.7426\n",
      "Epoch: 21 : Train Loss: 0.7290 | Valid loss: 0.7282\n",
      "Epoch: 22 : Train Loss: 0.7135 | Valid loss: 0.7123\n",
      "Epoch: 23 : Train Loss: 0.6982 | Valid loss: 0.6969\n",
      "Epoch: 24 : Train Loss: 0.6819 | Valid loss: 0.6829\n",
      "Epoch: 25 : Train Loss: 0.6672 | Valid loss: 0.6692\n",
      "Epoch: 26 : Train Loss: 0.6529 | Valid loss: 0.6575\n",
      "Epoch: 27 : Train Loss: 0.6392 | Valid loss: 0.6433\n",
      "Epoch: 28 : Train Loss: 0.6258 | Valid loss: 0.6367\n",
      "Epoch: 29 : Train Loss: 0.6149 | Valid loss: 0.6196\n",
      "Epoch: 30 : Train Loss: 0.6017 | Valid loss: 0.6148\n",
      "Epoch: 31 : Train Loss: 0.5908 | Valid loss: 0.6054\n",
      "Epoch: 32 : Train Loss: 0.5785 | Valid loss: 0.5992\n",
      "Epoch: 33 : Train Loss: 0.5697 | Valid loss: 0.5785\n",
      "Epoch: 34 : Train Loss: 0.5605 | Valid loss: 0.5754\n",
      "Epoch: 35 : Train Loss: 0.5504 | Valid loss: 0.5637\n",
      "Epoch: 36 : Train Loss: 0.5409 | Valid loss: 0.5560\n",
      "Epoch: 37 : Train Loss: 0.5326 | Valid loss: 0.5570\n",
      "Epoch: 38 : Train Loss: 0.5242 | Valid loss: 0.5486\n",
      "Epoch: 39 : Train Loss: 0.5157 | Valid loss: 0.5363\n",
      "Epoch: 40 : Train Loss: 0.5075 | Valid loss: 0.5394\n",
      "Epoch: 41 : Train Loss: 0.5006 | Valid loss: 0.5286\n",
      "Epoch: 42 : Train Loss: 0.4928 | Valid loss: 0.5192\n",
      "Epoch: 43 : Train Loss: 0.4850 | Valid loss: 0.5267\n",
      "Epoch: 44 : Train Loss: 0.4799 | Valid loss: 0.5131\n",
      "Epoch: 45 : Train Loss: 0.4713 | Valid loss: 0.5156\n",
      "Epoch: 46 : Train Loss: 0.4661 | Valid loss: 0.4961\n",
      "Epoch: 47 : Train Loss: 0.4602 | Valid loss: 0.4924\n",
      "Epoch: 48 : Train Loss: 0.4522 | Valid loss: 0.4873\n",
      "Epoch: 49 : Train Loss: 0.4447 | Valid loss: 0.5261\n",
      "Epoch: 50 : Train Loss: 0.4370 | Valid loss: 0.4840\n",
      "Epoch: 51 : Train Loss: 0.4339 | Valid loss: 0.4831\n",
      "Epoch: 52 : Train Loss: 0.4279 | Valid loss: 0.4694\n",
      "Epoch: 53 : Train Loss: 0.4214 | Valid loss: 0.4796\n",
      "Epoch: 54 : Train Loss: 0.4142 | Valid loss: 0.5013\n",
      "Epoch: 55 : Train Loss: 0.4114 | Valid loss: 0.4569\n",
      "Epoch: 56 : Train Loss: 0.4031 | Valid loss: 0.4584\n",
      "Epoch: 57 : Train Loss: 0.3986 | Valid loss: 0.4716\n",
      "Epoch: 58 : Train Loss: 0.3913 | Valid loss: 0.4418\n",
      "Epoch: 59 : Train Loss: 0.3866 | Valid loss: 0.4654\n",
      "Epoch: 60 : Train Loss: 0.3786 | Valid loss: 0.4362\n",
      "Epoch: 61 : Train Loss: 0.3729 | Valid loss: 0.4312\n",
      "Epoch: 62 : Train Loss: 0.3670 | Valid loss: 0.4561\n",
      "Epoch: 63 : Train Loss: 0.3649 | Valid loss: 0.4577\n",
      "Epoch: 64 : Train Loss: 0.3549 | Valid loss: 0.4427\n",
      "Epoch: 65 : Train Loss: 0.3546 | Valid loss: 0.4295\n",
      "Epoch: 66 : Train Loss: 0.3458 | Valid loss: 0.4190\n",
      "Epoch: 67 : Train Loss: 0.3409 | Valid loss: 0.4437\n",
      "Epoch: 68 : Train Loss: 0.3343 | Valid loss: 0.4338\n",
      "Epoch: 69 : Train Loss: 0.3303 | Valid loss: 0.4013\n",
      "Epoch: 70 : Train Loss: 0.3249 | Valid loss: 0.3960\n",
      "Epoch: 71 : Train Loss: 0.3142 | Valid loss: 0.3996\n",
      "Epoch: 72 : Train Loss: 0.3160 | Valid loss: 0.3958\n",
      "Epoch: 73 : Train Loss: 0.3072 | Valid loss: 0.3901\n",
      "Epoch: 74 : Train Loss: 0.3039 | Valid loss: 0.3812\n",
      "Epoch: 75 : Train Loss: 0.2932 | Valid loss: 0.3846\n",
      "Epoch: 76 : Train Loss: 0.2924 | Valid loss: 0.3918\n",
      "Epoch: 77 : Train Loss: 0.2840 | Valid loss: 0.3812\n",
      "Epoch: 78 : Train Loss: 0.2832 | Valid loss: 0.4493\n",
      "Epoch: 79 : Train Loss: 0.2727 | Valid loss: 0.3754\n",
      "Epoch: 80 : Train Loss: 0.2761 | Valid loss: 0.3959\n",
      "Epoch: 81 : Train Loss: 0.2648 | Valid loss: 0.4167\n",
      "Epoch: 82 : Train Loss: 0.2619 | Valid loss: 0.3732\n",
      "Epoch: 83 : Train Loss: 0.2526 | Valid loss: 0.3458\n",
      "Epoch: 84 : Train Loss: 0.2495 | Valid loss: 0.3845\n",
      "Epoch: 85 : Train Loss: 0.2552 | Valid loss: 0.3883\n",
      "Epoch: 86 : Train Loss: 0.2369 | Valid loss: 0.3382\n",
      "Epoch: 87 : Train Loss: 0.2303 | Valid loss: 0.3383\n",
      "Epoch: 88 : Train Loss: 0.2365 | Valid loss: 0.3282\n",
      "Epoch: 89 : Train Loss: 0.2211 | Valid loss: 0.3264\n",
      "Epoch: 90 : Train Loss: 0.2247 | Valid loss: 0.3264\n",
      "Epoch: 91 : Train Loss: 0.2132 | Valid loss: 0.3177\n",
      "Epoch: 92 : Train Loss: 0.2054 | Valid loss: 0.3185\n",
      "Epoch: 93 : Train Loss: 0.2014 | Valid loss: 0.3534\n",
      "Epoch: 94 : Train Loss: 0.2018 | Valid loss: 0.3438\n",
      "Epoch: 95 : Train Loss: 0.1947 | Valid loss: 0.3088\n",
      "Epoch: 96 : Train Loss: 0.1912 | Valid loss: 0.3314\n",
      "Epoch: 97 : Train Loss: 0.1888 | Valid loss: 0.2935\n",
      "Epoch: 98 : Train Loss: 0.1808 | Valid loss: 0.3955\n",
      "Epoch: 99 : Train Loss: 0.1743 | Valid loss: 0.3523\n",
      "Epoch: 100 : Train Loss: 0.1816 | Valid loss: 0.2856\n",
      "Epoch: 101 : Train Loss: 0.1716 | Valid loss: 0.2817\n",
      "Epoch: 102 : Train Loss: 0.1601 | Valid loss: 0.3607\n",
      "Epoch: 103 : Train Loss: 0.1597 | Valid loss: 0.2717\n",
      "Epoch: 104 : Train Loss: 0.1572 | Valid loss: 0.2888\n",
      "Epoch: 105 : Train Loss: 0.1525 | Valid loss: 0.2661\n",
      "Epoch: 106 : Train Loss: 0.1509 | Valid loss: 0.2661\n",
      "Epoch: 107 : Train Loss: 0.1409 | Valid loss: 0.2637\n",
      "Epoch: 108 : Train Loss: 0.1395 | Valid loss: 0.4651\n",
      "Epoch: 109 : Train Loss: 0.1395 | Valid loss: 0.2715\n",
      "Epoch: 110 : Train Loss: 0.1505 | Valid loss: 0.2727\n",
      "Epoch: 111 : Train Loss: 0.1292 | Valid loss: 0.2503\n",
      "Epoch: 112 : Train Loss: 0.1265 | Valid loss: 0.2535\n",
      "Epoch: 113 : Train Loss: 0.1218 | Valid loss: 0.2898\n",
      "Epoch: 114 : Train Loss: 0.1207 | Valid loss: 0.2468\n",
      "Epoch: 115 : Train Loss: 0.1178 | Valid loss: 0.2453\n",
      "Epoch: 116 : Train Loss: 0.1096 | Valid loss: 0.2394\n",
      "Epoch: 117 : Train Loss: 0.1112 | Valid loss: 0.2560\n",
      "Epoch: 118 : Train Loss: 0.1054 | Valid loss: 0.2406\n",
      "Epoch: 119 : Train Loss: 0.1048 | Valid loss: 0.2697\n",
      "Epoch: 120 : Train Loss: 0.1192 | Valid loss: 0.2504\n",
      "Epoch: 121 : Train Loss: 0.1027 | Valid loss: 0.3743\n",
      "Epoch: 122 : Train Loss: 0.0952 | Valid loss: 0.2264\n",
      "Epoch: 123 : Train Loss: 0.0968 | Valid loss: 0.2295\n",
      "Epoch: 124 : Train Loss: 0.0934 | Valid loss: 0.2404\n",
      "Epoch: 125 : Train Loss: 0.1063 | Valid loss: 0.2876\n",
      "Epoch: 126 : Train Loss: 0.0905 | Valid loss: 0.2502\n",
      "Epoch: 127 : Train Loss: 0.0867 | Valid loss: 0.2288\n",
      "Epoch: 128 : Train Loss: 0.0850 | Valid loss: 0.2644\n",
      "Epoch: 129 : Train Loss: 0.0810 | Valid loss: 0.2239\n",
      "Epoch: 130 : Train Loss: 0.0893 | Valid loss: 0.2184\n",
      "Epoch: 131 : Train Loss: 0.0763 | Valid loss: 0.2227\n",
      "Epoch: 132 : Train Loss: 0.0818 | Valid loss: 0.2203\n",
      "Epoch: 133 : Train Loss: 0.0889 | Valid loss: 0.2198\n",
      "Epoch: 134 : Train Loss: 0.0738 | Valid loss: 0.2238\n",
      "Epoch: 135 : Train Loss: 0.0747 | Valid loss: 0.2291\n",
      "Epoch: 136 : Train Loss: 0.0690 | Valid loss: 0.2226\n",
      "Epoch: 137 : Train Loss: 0.0678 | Valid loss: 0.2204\n",
      "Epoch: 138 : Train Loss: 0.0721 | Valid loss: 0.2121\n",
      "Epoch: 139 : Train Loss: 0.0955 | Valid loss: 0.2143\n",
      "Epoch: 140 : Train Loss: 0.0660 | Valid loss: 0.2257\n",
      "Epoch: 141 : Train Loss: 0.0645 | Valid loss: 0.2329\n",
      "Epoch: 142 : Train Loss: 0.0649 | Valid loss: 0.2441\n",
      "Epoch: 143 : Train Loss: 0.0612 | Valid loss: 0.2208\n",
      "Epoch: 144 : Train Loss: 0.0614 | Valid loss: 0.2102\n",
      "Epoch: 145 : Train Loss: 0.0602 | Valid loss: 0.2348\n",
      "Epoch: 146 : Train Loss: 0.0584 | Valid loss: 0.2094\n",
      "Epoch: 147 : Train Loss: 0.0576 | Valid loss: 0.2248\n",
      "Epoch: 148 : Train Loss: 0.0611 | Valid loss: 0.2499\n",
      "Epoch: 149 : Train Loss: 0.0589 | Valid loss: 0.2137\n",
      "Epoch: 150 : Train Loss: 0.0560 | Valid loss: 0.2111\n",
      "Epoch: 151 : Train Loss: 0.0562 | Valid loss: 0.2589\n",
      "Epoch: 152 : Train Loss: 0.0522 | Valid loss: 0.2373\n",
      "Epoch: 153 : Train Loss: 0.0573 | Valid loss: 0.2087\n",
      "Epoch: 154 : Train Loss: 0.0667 | Valid loss: 0.2398\n",
      "Epoch: 155 : Train Loss: 0.0546 | Valid loss: 0.2111\n",
      "Epoch: 156 : Train Loss: 0.0514 | Valid loss: 0.2139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 157 : Train Loss: 0.0535 | Valid loss: 0.2320\n",
      "Epoch: 158 : Train Loss: 0.0506 | Valid loss: 0.2145\n",
      "Epoch: 159 : Train Loss: 0.0797 | Valid loss: 0.2174\n",
      "Epoch: 160 : Train Loss: 0.0509 | Valid loss: 0.2162\n",
      "Epoch: 161 : Train Loss: 0.0498 | Valid loss: 0.2181\n",
      "Epoch: 162 : Train Loss: 0.0489 | Valid loss: 0.2223\n",
      "Epoch: 163 : Train Loss: 0.0670 | Valid loss: 0.2138\n",
      "Epoch: 164 : Train Loss: 0.0477 | Valid loss: 0.2137\n",
      "Epoch: 165 : Train Loss: 0.0466 | Valid loss: 0.2189\n",
      "Epoch: 166 : Train Loss: 0.0463 | Valid loss: 0.2714\n",
      "Epoch: 167 : Train Loss: 0.0474 | Valid loss: 0.2130\n",
      "Epoch: 168 : Train Loss: 0.0470 | Valid loss: 0.2303\n",
      "Epoch: 169 : Train Loss: 0.0470 | Valid loss: 0.2114\n",
      "Epoch: 170 : Train Loss: 0.0455 | Valid loss: 0.2214\n",
      "Epoch: 171 : Train Loss: 0.0537 | Valid loss: 0.2143\n",
      "Epoch: 172 : Train Loss: 0.0475 | Valid loss: 0.2241\n",
      "Epoch: 173 : Train Loss: 0.0447 | Valid loss: 0.2119\n",
      "Epoch: 174 : Train Loss: 0.0449 | Valid loss: 0.2847\n",
      "Epoch: 175 : Train Loss: 0.0459 | Valid loss: 0.2120\n",
      "Epoch: 176 : Train Loss: 0.0441 | Valid loss: 0.2103\n",
      "Epoch: 177 : Train Loss: 0.0444 | Valid loss: 0.2375\n",
      "Epoch: 178 : Train Loss: 0.0426 | Valid loss: 0.2231\n",
      "Epoch: 179 : Train Loss: 0.0436 | Valid loss: 0.2143\n",
      "Epoch: 180 : Train Loss: 0.0434 | Valid loss: 0.2266\n",
      "Epoch: 181 : Train Loss: 0.0686 | Valid loss: 0.2439\n",
      "Epoch: 182 : Train Loss: 0.0435 | Valid loss: 0.2193\n",
      "Epoch: 183 : Train Loss: 0.0418 | Valid loss: 0.2129\n",
      "Epoch: 184 : Train Loss: 0.0426 | Valid loss: 0.2097\n",
      "Epoch: 185 : Train Loss: 0.0408 | Valid loss: 0.2503\n",
      "Epoch: 186 : Train Loss: 0.0413 | Valid loss: 0.2372\n",
      "Epoch: 187 : Train Loss: 0.0401 | Valid loss: 0.2234\n",
      "Epoch: 188 : Train Loss: 0.0405 | Valid loss: 0.2191\n",
      "Epoch: 189 : Train Loss: 0.0406 | Valid loss: 0.2260\n",
      "Epoch: 190 : Train Loss: 0.0895 | Valid loss: 0.5523\n",
      "Epoch: 191 : Train Loss: 0.0429 | Valid loss: 0.2158\n",
      "Epoch: 192 : Train Loss: 0.0410 | Valid loss: 0.2316\n",
      "Epoch: 193 : Train Loss: 0.0417 | Valid loss: 0.2133\n",
      "Epoch: 194 : Train Loss: 0.0394 | Valid loss: 0.2176\n",
      "Epoch: 195 : Train Loss: 0.0402 | Valid loss: 0.2182\n",
      "Epoch: 196 : Train Loss: 0.0397 | Valid loss: 0.2146\n",
      "Epoch: 197 : Train Loss: 0.0379 | Valid loss: 0.2306\n",
      "Epoch: 198 : Train Loss: 0.0396 | Valid loss: 0.2217\n",
      "Epoch: 199 : Train Loss: 0.0395 | Valid loss: 0.2152\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss, preds_list, label_list = SGD(tr_X, te_X, \n",
    "                             n_epoches=200, \n",
    "                             lr=0.007, \n",
    "                             dropout=0.2, \n",
    "                             batch_size = 64, \n",
    "                             num_hidden_nodes = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch = [i for i in range(len(train_loss))]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epoch, train_loss)\n",
    "plt.plot(epoch, valid_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9379092609915809\n",
      "Precision: 0.9408597034355329\n",
      "Recall: 0.9409344702808645\n",
      "F1-Score: 0.9408604905562413\n"
     ]
    }
   ],
   "source": [
    "Y_te = label_list\n",
    "preds_te = preds_list\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    ">1. choose SGD rather than Adam.\n",
    ">2. using nn.CrossEntropyLoss() (nn.logSoftmax() + nn.NLLLoss()) for multi-class classification.\n",
    "\n",
    "tutorial: https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
